{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Graph Embeddings Library","text":"<p>Welcome to the Graph Embeddings library documentation! This library provides a comprehensive suite of tools for embedding graphs in various geometric spaces and performing graph algorithms that leverage these embeddings.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Multiple embedding types:<ul> <li>Euclidean space embeddings</li> <li>Spherical surface embeddings</li> <li>Hyperbolic (Poincar\u00e9 disk) embeddings</li> </ul> </li> <li>Embedding-aware graph algorithms</li> <li>Interactive visualization</li> <li>Support for both directed and undirected graphs</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>from core.embeddings import EuclideanEmbedding\nfrom core.algorithms import DijkstraEmbedding\nimport networkx as nx\n\n# Create a sample graph\nG = nx.karate_club_graph()\n\n# Create and train the embedding\nembedding = EuclideanEmbedding(dim=2)\nembedding.train(G)\n\n# Use embedding-aware algorithms\ndijkstra = DijkstraEmbedding(embedding)\npath = dijkstra.shortest_path(0, 33)\n</code></pre>"},{"location":"#why-graph-embeddings","title":"Why Graph Embeddings?","text":"<p>Graph embeddings provide a way to represent graph nodes in continuous vector spaces while preserving graph properties such as node similarity and graph structure. This enables:</p> <ol> <li>Efficient Algorithms: Use geometric properties to speed up graph algorithms</li> <li>Visualization: Natural way to visualize high-dimensional graph data</li> <li>Machine Learning: Bridge between graph structures and traditional ML methods</li> </ol>"},{"location":"#project-structure","title":"Project Structure","text":"<pre><code>graph-embeddings/\n\u251c\u2500\u2500 core/\n\u2502   \u251c\u2500\u2500 embeddings/       # Embedding implementations\n\u2502   \u251c\u2500\u2500 algorithms/       # Graph algorithms\n\u2502   \u2514\u2500\u2500 visualization/    # Visualization tools\n\u251c\u2500\u2500 docs/                 # Documentation\n\u2514\u2500\u2500 examples/            # Usage examples\n</code></pre>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions! Please see our Contributing Guide for details.</p>"},{"location":"#license","title":"License","text":"<p>This project is licensed under the MIT License - see the LICENSE file for details.</p>"},{"location":"algorithms/overview/","title":"Graph Algorithms with Embeddings","text":"<p>This section explains how graph embeddings can be used to accelerate traditional graph algorithms. Graph embeddings are low-dimensional vector representations of nodes that capture the structural properties of the graph. By using these embeddings, we can often transform complex graph operations into simple vector operations, leading to significant performance improvements.</p>"},{"location":"algorithms/overview/#algorithm-overview","title":"Algorithm Overview","text":"Algorithm Traditional Approach Embedding-Based Approach Speed Improvement Personalized PageRank Matrix iterations Vector similarity 5-10x Node Classification Graph traversal Neural network 2-5x Community Detection Graph clustering Vector clustering 10-20x Link Prediction Path-based metrics Vector operations 3-8x"},{"location":"algorithms/overview/#how-embeddings-improve-performance","title":"How Embeddings Improve Performance","text":""},{"location":"algorithms/overview/#space-efficiency","title":"Space Efficiency","text":"<ul> <li>Dimension Reduction: Instead of storing the full adjacency matrix (O(n\u00b2)), embeddings store a fixed-size vector per node (O(n\u00b7d) where d &lt;&lt; n)</li> <li>Memory Locality: Vector operations have better cache utilization compared to graph traversal</li> <li>Sparse to Dense: Convert sparse graph operations to dense matrix operations, which are more efficient on modern hardware</li> </ul>"},{"location":"algorithms/overview/#time-efficiency","title":"Time Efficiency","text":"<ul> <li>Parallel Processing: Vector operations are highly parallelizable on modern CPUs/GPUs</li> <li>Approximation: Trade exact solutions for very good approximations with much better scaling</li> <li>Precomputation: Most complex graph structure information is captured during the embedding process</li> </ul>"},{"location":"algorithms/overview/#detailed-algorithm-descriptions","title":"Detailed Algorithm Descriptions","text":""},{"location":"algorithms/overview/#personalized-pagerank","title":"Personalized PageRank","text":"<p>               Bases: <code>EmbeddingAwareAlgorithm</code></p> <p>Fast approximate Personalized PageRank using embedding space.</p> Source code in <code>core/algorithms/embedding_algorithms.py</code> <pre><code>class ApproximatePersonalizedPageRank(EmbeddingAwareAlgorithm):\n    \"\"\"Fast approximate Personalized PageRank using embedding space.\"\"\"\n\n    @EmbeddingAwareAlgorithm.measure_execution_time\n    def run(self, source_node: int, num_neighbors: int = 10) -&gt; Dict[int, float]:\n        \"\"\"\n        Compute approximate Personalized PageRank scores using embedding space similarity.\n        This is much faster than traditional PPR computation for large graphs.\n\n        Args:\n            source_node: Starting node for PPR computation\n            num_neighbors: Number of nearest neighbors to consider in embedding space\n\n        Returns:\n            Dictionary mapping node IDs to their PPR scores\n        \"\"\"\n        # Get embeddings for all nodes\n        nodes = list(self.graph.nodes())\n        embeddings = np.array([self.embedder.get_embedding(node) for node in nodes])\n        source_embedding = self.embedder.get_embedding(source_node)\n\n        # Find nearest neighbors in embedding space\n        nbrs = NearestNeighbors(n_neighbors=num_neighbors, algorithm='ball_tree').fit(embeddings)\n        distances, indices = nbrs.kneighbors([source_embedding])\n\n        # Compute approximate PPR scores based on embedding similarity\n        scores = {}\n        max_distance = np.max(distances[0]) + 1e-6  # Avoid division by zero\n        for node, distance in zip(np.array(nodes)[indices[0]], distances[0]):\n            scores[node] = 1.0 - (distance / max_distance)\n\n        return scores\n</code></pre> <p>How Embeddings Help: Instead of iterative matrix multiplication (O(m\u00b7k) for k iterations), we use vector similarity computations (O(n\u00b7d)) where d is the embedding dimension. This is especially effective for large, sparse graphs.</p>"},{"location":"algorithms/overview/#core.algorithms.embedding_algorithms.ApproximatePersonalizedPageRank.run","title":"<code>run(source_node, num_neighbors=10)</code>","text":"<p>Compute approximate Personalized PageRank scores using embedding space similarity. This is much faster than traditional PPR computation for large graphs.</p> <p>Parameters:</p> Name Type Description Default <code>source_node</code> <code>int</code> <p>Starting node for PPR computation</p> required <code>num_neighbors</code> <code>int</code> <p>Number of nearest neighbors to consider in embedding space</p> <code>10</code> <p>Returns:</p> Type Description <code>Dict[int, float]</code> <p>Dictionary mapping node IDs to their PPR scores</p> Source code in <code>core/algorithms/embedding_algorithms.py</code> <pre><code>@EmbeddingAwareAlgorithm.measure_execution_time\ndef run(self, source_node: int, num_neighbors: int = 10) -&gt; Dict[int, float]:\n    \"\"\"\n    Compute approximate Personalized PageRank scores using embedding space similarity.\n    This is much faster than traditional PPR computation for large graphs.\n\n    Args:\n        source_node: Starting node for PPR computation\n        num_neighbors: Number of nearest neighbors to consider in embedding space\n\n    Returns:\n        Dictionary mapping node IDs to their PPR scores\n    \"\"\"\n    # Get embeddings for all nodes\n    nodes = list(self.graph.nodes())\n    embeddings = np.array([self.embedder.get_embedding(node) for node in nodes])\n    source_embedding = self.embedder.get_embedding(source_node)\n\n    # Find nearest neighbors in embedding space\n    nbrs = NearestNeighbors(n_neighbors=num_neighbors, algorithm='ball_tree').fit(embeddings)\n    distances, indices = nbrs.kneighbors([source_embedding])\n\n    # Compute approximate PPR scores based on embedding similarity\n    scores = {}\n    max_distance = np.max(distances[0]) + 1e-6  # Avoid division by zero\n    for node, distance in zip(np.array(nodes)[indices[0]], distances[0]):\n        scores[node] = 1.0 - (distance / max_distance)\n\n    return scores\n</code></pre>"},{"location":"algorithms/overview/#node-classification","title":"Node Classification","text":"<p>               Bases: <code>EmbeddingAwareAlgorithm</code></p> <p>Efficient node classification using embedding space.</p> Source code in <code>core/algorithms/embedding_algorithms.py</code> <pre><code>class FastNodeClassification(EmbeddingAwareAlgorithm):\n    \"\"\"Efficient node classification using embedding space.\"\"\"\n\n    @EmbeddingAwareAlgorithm.measure_execution_time\n    def run(self, labeled_nodes: Dict[int, str], k: int = 5) -&gt; Dict[int, str]:\n        \"\"\"\n        Classify nodes based on their k-nearest labeled neighbors in embedding space.\n        Much faster than traditional label propagation algorithms.\n\n        Args:\n            labeled_nodes: Dictionary mapping node IDs to their labels\n            k: Number of nearest neighbors to consider\n\n        Returns:\n            Dictionary mapping all node IDs to predicted labels\n        \"\"\"\n        # Get embeddings\n        all_nodes = list(self.graph.nodes())\n        all_embeddings = np.array([self.embedder.get_embedding(node) for node in all_nodes])\n\n        # Create labeled embeddings dataset\n        labeled_indices = [i for i, node in enumerate(all_nodes) if node in labeled_nodes]\n        labeled_embeddings = all_embeddings[labeled_indices]\n        labels = [labeled_nodes[all_nodes[i]] for i in labeled_indices]\n\n        # Find k-nearest neighbors for all unlabeled nodes\n        nbrs = NearestNeighbors(n_neighbors=min(k, len(labeled_indices)), \n                               algorithm='ball_tree').fit(labeled_embeddings)\n\n        predictions = {}\n        for node, embedding in zip(all_nodes, all_embeddings):\n            if node in labeled_nodes:\n                predictions[node] = labeled_nodes[node]\n            else:\n                distances, indices = nbrs.kneighbors([embedding])\n                # Majority voting weighted by inverse distance\n                weights = 1.0 / (distances[0] + 1e-6)\n                neighbor_labels = [labels[i] for i in indices[0]]\n                unique_labels = set(neighbor_labels)\n                label_scores = {label: sum(weights[i] for i, l in enumerate(neighbor_labels) if l == label)\n                              for label in unique_labels}\n                predictions[node] = max(label_scores.items(), key=lambda x: x[1])[0]\n\n        return predictions\n</code></pre> <p>How Embeddings Help: Rather than expensive graph feature extraction and neighbor traversal, we use the embedding vectors as ready-to-use feature vectors for classification, reducing the complexity from O(n\u00b7k\u00b7m) to O(n\u00b7d).</p>"},{"location":"algorithms/overview/#core.algorithms.embedding_algorithms.FastNodeClassification.run","title":"<code>run(labeled_nodes, k=5)</code>","text":"<p>Classify nodes based on their k-nearest labeled neighbors in embedding space. Much faster than traditional label propagation algorithms.</p> <p>Parameters:</p> Name Type Description Default <code>labeled_nodes</code> <code>Dict[int, str]</code> <p>Dictionary mapping node IDs to their labels</p> required <code>k</code> <code>int</code> <p>Number of nearest neighbors to consider</p> <code>5</code> <p>Returns:</p> Type Description <code>Dict[int, str]</code> <p>Dictionary mapping all node IDs to predicted labels</p> Source code in <code>core/algorithms/embedding_algorithms.py</code> <pre><code>@EmbeddingAwareAlgorithm.measure_execution_time\ndef run(self, labeled_nodes: Dict[int, str], k: int = 5) -&gt; Dict[int, str]:\n    \"\"\"\n    Classify nodes based on their k-nearest labeled neighbors in embedding space.\n    Much faster than traditional label propagation algorithms.\n\n    Args:\n        labeled_nodes: Dictionary mapping node IDs to their labels\n        k: Number of nearest neighbors to consider\n\n    Returns:\n        Dictionary mapping all node IDs to predicted labels\n    \"\"\"\n    # Get embeddings\n    all_nodes = list(self.graph.nodes())\n    all_embeddings = np.array([self.embedder.get_embedding(node) for node in all_nodes])\n\n    # Create labeled embeddings dataset\n    labeled_indices = [i for i, node in enumerate(all_nodes) if node in labeled_nodes]\n    labeled_embeddings = all_embeddings[labeled_indices]\n    labels = [labeled_nodes[all_nodes[i]] for i in labeled_indices]\n\n    # Find k-nearest neighbors for all unlabeled nodes\n    nbrs = NearestNeighbors(n_neighbors=min(k, len(labeled_indices)), \n                           algorithm='ball_tree').fit(labeled_embeddings)\n\n    predictions = {}\n    for node, embedding in zip(all_nodes, all_embeddings):\n        if node in labeled_nodes:\n            predictions[node] = labeled_nodes[node]\n        else:\n            distances, indices = nbrs.kneighbors([embedding])\n            # Majority voting weighted by inverse distance\n            weights = 1.0 / (distances[0] + 1e-6)\n            neighbor_labels = [labels[i] for i in indices[0]]\n            unique_labels = set(neighbor_labels)\n            label_scores = {label: sum(weights[i] for i, l in enumerate(neighbor_labels) if l == label)\n                          for label in unique_labels}\n            predictions[node] = max(label_scores.items(), key=lambda x: x[1])[0]\n\n    return predictions\n</code></pre>"},{"location":"algorithms/overview/#community-detection","title":"Community Detection","text":"<p>               Bases: <code>EmbeddingAwareAlgorithm</code></p> <p>Fast community detection using embedding space clustering.</p> Source code in <code>core/algorithms/embedding_algorithms.py</code> <pre><code>class EfficientCommunityDetection(EmbeddingAwareAlgorithm):\n    \"\"\"Fast community detection using embedding space clustering.\"\"\"\n\n    @EmbeddingAwareAlgorithm.measure_execution_time\n    def run(self, min_community_size: int = 5, eps: float = 0.5) -&gt; List[Set[int]]:\n        \"\"\"\n        Detect communities by clustering nodes in embedding space using DBSCAN.\n        Much faster than traditional community detection algorithms for large graphs.\n\n        Args:\n            min_community_size: Minimum number of nodes in a community\n            eps: Maximum distance between nodes in the same neighborhood\n\n        Returns:\n            List of sets, where each set contains node IDs in the same community\n        \"\"\"\n        # Get embeddings for all nodes\n        nodes = list(self.graph.nodes())\n        embeddings = np.array([self.embedder.get_embedding(node) for node in nodes])\n\n        # Perform DBSCAN clustering\n        clustering = DBSCAN(eps=eps, min_samples=min_community_size).fit(embeddings)\n        labels = clustering.labels_\n\n        # Group nodes by cluster\n        communities = {}\n        for node, label in zip(nodes, labels):\n            if label != -1:  # Ignore noise points\n                if label not in communities:\n                    communities[label] = set()\n                communities[label].add(node)\n\n        return list(communities.values())\n</code></pre> <p>How Embeddings Help: Traditional community detection often requires examining the entire graph structure repeatedly. With embeddings, we can cluster the embedding vectors directly, reducing complexity from O(n\u00b2\u00b7log(n)) to O(n\u00b7log(n)).</p>"},{"location":"algorithms/overview/#core.algorithms.embedding_algorithms.EfficientCommunityDetection.run","title":"<code>run(min_community_size=5, eps=0.5)</code>","text":"<p>Detect communities by clustering nodes in embedding space using DBSCAN. Much faster than traditional community detection algorithms for large graphs.</p> <p>Parameters:</p> Name Type Description Default <code>min_community_size</code> <code>int</code> <p>Minimum number of nodes in a community</p> <code>5</code> <code>eps</code> <code>float</code> <p>Maximum distance between nodes in the same neighborhood</p> <code>0.5</code> <p>Returns:</p> Type Description <code>List[Set[int]]</code> <p>List of sets, where each set contains node IDs in the same community</p> Source code in <code>core/algorithms/embedding_algorithms.py</code> <pre><code>@EmbeddingAwareAlgorithm.measure_execution_time\ndef run(self, min_community_size: int = 5, eps: float = 0.5) -&gt; List[Set[int]]:\n    \"\"\"\n    Detect communities by clustering nodes in embedding space using DBSCAN.\n    Much faster than traditional community detection algorithms for large graphs.\n\n    Args:\n        min_community_size: Minimum number of nodes in a community\n        eps: Maximum distance between nodes in the same neighborhood\n\n    Returns:\n        List of sets, where each set contains node IDs in the same community\n    \"\"\"\n    # Get embeddings for all nodes\n    nodes = list(self.graph.nodes())\n    embeddings = np.array([self.embedder.get_embedding(node) for node in nodes])\n\n    # Perform DBSCAN clustering\n    clustering = DBSCAN(eps=eps, min_samples=min_community_size).fit(embeddings)\n    labels = clustering.labels_\n\n    # Group nodes by cluster\n    communities = {}\n    for node, label in zip(nodes, labels):\n        if label != -1:  # Ignore noise points\n            if label not in communities:\n                communities[label] = set()\n            communities[label].add(node)\n\n    return list(communities.values())\n</code></pre>"},{"location":"algorithms/overview/#link-prediction","title":"Link Prediction","text":"<p>               Bases: <code>EmbeddingAwareAlgorithm</code></p> <p>Fast link prediction using embedding similarity.</p> Source code in <code>core/algorithms/embedding_algorithms.py</code> <pre><code>class LinkPrediction(EmbeddingAwareAlgorithm):\n    \"\"\"Fast link prediction using embedding similarity.\"\"\"\n\n    @EmbeddingAwareAlgorithm.measure_execution_time\n    def run(self, source_node: int, num_candidates: int = 10) -&gt; List[Tuple[int, float]]:\n        \"\"\"\n        Predict most likely new edges for a given node using embedding similarity.\n        Much faster than traditional link prediction methods.\n\n        Args:\n            source_node: Node to predict new links for\n            num_candidates: Number of candidates to return\n\n        Returns:\n            List of tuples (node_id, score) for most likely new edges\n        \"\"\"\n        # Get embeddings\n        nodes = list(self.graph.nodes())\n        embeddings = np.array([self.embedder.get_embedding(node) for node in nodes])\n        source_embedding = self.embedder.get_embedding(source_node)\n\n        # Find nearest neighbors in embedding space\n        nbrs = NearestNeighbors(n_neighbors=num_candidates + 1, algorithm='ball_tree').fit(embeddings)\n        distances, indices = nbrs.kneighbors([source_embedding])\n\n        # Filter out existing edges and self-loops\n        existing_edges = set(self.graph.neighbors(source_node))\n        candidates = []\n        for node_idx, distance in zip(indices[0], distances[0]):\n            node = nodes[node_idx]\n            if node != source_node and node not in existing_edges:\n                similarity = 1.0 / (1.0 + distance)  # Convert distance to similarity score\n                candidates.append((node, similarity))\n\n        # Sort by similarity score\n        candidates.sort(key=lambda x: x[1], reverse=True)\n        return candidates[:num_candidates]\n</code></pre> <p>How Embeddings Help: Instead of computing path-based metrics (O(n\u00b3)), we can use vector operations on node embeddings (O(n\u00b7d\u00b2)) to predict potential links.</p>"},{"location":"algorithms/overview/#core.algorithms.embedding_algorithms.LinkPrediction.run","title":"<code>run(source_node, num_candidates=10)</code>","text":"<p>Predict most likely new edges for a given node using embedding similarity. Much faster than traditional link prediction methods.</p> <p>Parameters:</p> Name Type Description Default <code>source_node</code> <code>int</code> <p>Node to predict new links for</p> required <code>num_candidates</code> <code>int</code> <p>Number of candidates to return</p> <code>10</code> <p>Returns:</p> Type Description <code>List[Tuple[int, float]]</code> <p>List of tuples (node_id, score) for most likely new edges</p> Source code in <code>core/algorithms/embedding_algorithms.py</code> <pre><code>@EmbeddingAwareAlgorithm.measure_execution_time\ndef run(self, source_node: int, num_candidates: int = 10) -&gt; List[Tuple[int, float]]:\n    \"\"\"\n    Predict most likely new edges for a given node using embedding similarity.\n    Much faster than traditional link prediction methods.\n\n    Args:\n        source_node: Node to predict new links for\n        num_candidates: Number of candidates to return\n\n    Returns:\n        List of tuples (node_id, score) for most likely new edges\n    \"\"\"\n    # Get embeddings\n    nodes = list(self.graph.nodes())\n    embeddings = np.array([self.embedder.get_embedding(node) for node in nodes])\n    source_embedding = self.embedder.get_embedding(source_node)\n\n    # Find nearest neighbors in embedding space\n    nbrs = NearestNeighbors(n_neighbors=num_candidates + 1, algorithm='ball_tree').fit(embeddings)\n    distances, indices = nbrs.kneighbors([source_embedding])\n\n    # Filter out existing edges and self-loops\n    existing_edges = set(self.graph.neighbors(source_node))\n    candidates = []\n    for node_idx, distance in zip(indices[0], distances[0]):\n        node = nodes[node_idx]\n        if node != source_node and node not in existing_edges:\n            similarity = 1.0 / (1.0 + distance)  # Convert distance to similarity score\n            candidates.append((node, similarity))\n\n    # Sort by similarity score\n    candidates.sort(key=lambda x: x[1], reverse=True)\n    return candidates[:num_candidates]\n</code></pre>"},{"location":"api/reference/","title":"API Reference","text":"<p>This section provides detailed API documentation for all components of the Graph Embeddings library.</p>"},{"location":"api/reference/#embeddings","title":"Embeddings","text":""},{"location":"api/reference/#base-embedding","title":"Base Embedding","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all embedding types.</p> Source code in <code>core/embeddings/base.py</code> <pre><code>class BaseEmbedding(ABC):\n    \"\"\"Base class for all embedding types.\"\"\"\n\n    def __init__(self, dim: int = 2):\n        \"\"\"Initialize embedding parameters.\"\"\"\n        self.dim = dim\n        self.embeddings = {}\n\n    def train(self, graph: nx.Graph, iterations: int = 50) -&gt; None:\n        \"\"\"Train the embeddings on a graph.\"\"\"\n        # Initialize embeddings for all nodes\n        init_embeddings = self._initialize_embeddings(graph)\n\n        # Ensure all nodes have embeddings\n        for node in graph.nodes():\n            if node not in init_embeddings:\n                # Initialize missing nodes with random values\n                init_embeddings[node] = np.random.normal(0, 0.1, self.dim)\n\n        self.embeddings = init_embeddings\n\n        # Update embeddings\n        self._update_embeddings(graph, iterations)\n\n        # Final check to ensure all nodes have embeddings\n        for node in graph.nodes():\n            if node not in self.embeddings:\n                raise ValueError(f\"Node {node} missing from embeddings after training\")\n\n    @abstractmethod\n    def _initialize_embeddings(self, graph: nx.Graph) -&gt; Dict[int, np.ndarray]:\n        \"\"\"Initialize embeddings for all nodes in the graph.\"\"\"\n        pass\n\n    @abstractmethod\n    def _update_embeddings(self, graph: nx.Graph, iterations: int) -&gt; None:\n        \"\"\"Update embeddings using graph structure.\"\"\"\n        pass\n\n    def get_embedding(self, node: int) -&gt; np.ndarray:\n        \"\"\"Get the embedding vector for a node.\"\"\"\n        return self.embeddings[node]\n\n    @abstractmethod\n    def compute_distance(self, node1: int, node2: int) -&gt; float:\n        \"\"\"Compute distance between two nodes in the embedding space.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_visualization_coords(self, node: int) -&gt; Tuple[float, float]:\n        \"\"\"Get 2D coordinates for visualization.\"\"\"\n        pass\n</code></pre>"},{"location":"api/reference/#core.embeddings.base.BaseEmbedding.__init__","title":"<code>__init__(dim=2)</code>","text":"<p>Initialize embedding parameters.</p> Source code in <code>core/embeddings/base.py</code> <pre><code>def __init__(self, dim: int = 2):\n    \"\"\"Initialize embedding parameters.\"\"\"\n    self.dim = dim\n    self.embeddings = {}\n</code></pre>"},{"location":"api/reference/#core.embeddings.base.BaseEmbedding.compute_distance","title":"<code>compute_distance(node1, node2)</code>  <code>abstractmethod</code>","text":"<p>Compute distance between two nodes in the embedding space.</p> Source code in <code>core/embeddings/base.py</code> <pre><code>@abstractmethod\ndef compute_distance(self, node1: int, node2: int) -&gt; float:\n    \"\"\"Compute distance between two nodes in the embedding space.\"\"\"\n    pass\n</code></pre>"},{"location":"api/reference/#core.embeddings.base.BaseEmbedding.get_embedding","title":"<code>get_embedding(node)</code>","text":"<p>Get the embedding vector for a node.</p> Source code in <code>core/embeddings/base.py</code> <pre><code>def get_embedding(self, node: int) -&gt; np.ndarray:\n    \"\"\"Get the embedding vector for a node.\"\"\"\n    return self.embeddings[node]\n</code></pre>"},{"location":"api/reference/#core.embeddings.base.BaseEmbedding.get_visualization_coords","title":"<code>get_visualization_coords(node)</code>  <code>abstractmethod</code>","text":"<p>Get 2D coordinates for visualization.</p> Source code in <code>core/embeddings/base.py</code> <pre><code>@abstractmethod\ndef get_visualization_coords(self, node: int) -&gt; Tuple[float, float]:\n    \"\"\"Get 2D coordinates for visualization.\"\"\"\n    pass\n</code></pre>"},{"location":"api/reference/#core.embeddings.base.BaseEmbedding.train","title":"<code>train(graph, iterations=50)</code>","text":"<p>Train the embeddings on a graph.</p> Source code in <code>core/embeddings/base.py</code> <pre><code>def train(self, graph: nx.Graph, iterations: int = 50) -&gt; None:\n    \"\"\"Train the embeddings on a graph.\"\"\"\n    # Initialize embeddings for all nodes\n    init_embeddings = self._initialize_embeddings(graph)\n\n    # Ensure all nodes have embeddings\n    for node in graph.nodes():\n        if node not in init_embeddings:\n            # Initialize missing nodes with random values\n            init_embeddings[node] = np.random.normal(0, 0.1, self.dim)\n\n    self.embeddings = init_embeddings\n\n    # Update embeddings\n    self._update_embeddings(graph, iterations)\n\n    # Final check to ensure all nodes have embeddings\n    for node in graph.nodes():\n        if node not in self.embeddings:\n            raise ValueError(f\"Node {node} missing from embeddings after training\")\n</code></pre>"},{"location":"api/reference/#euclidean-embedding","title":"Euclidean Embedding","text":"<p>               Bases: <code>BaseEmbedding</code></p> <p>Euclidean space embedding using graph structure.</p> Source code in <code>core/embeddings/euclidean.py</code> <pre><code>class EuclideanEmbedding(BaseEmbedding):\n    \"\"\"Euclidean space embedding using graph structure.\"\"\"\n\n    def __init__(self, dim: int = 2):\n        \"\"\"Initialize embedding with dimension.\"\"\"\n        super().__init__(dim)\n\n    def _initialize_embeddings(self, graph: nx.Graph) -&gt; Dict[int, np.ndarray]:\n        \"\"\"Initialize node embeddings using spectral layout for better starting positions.\"\"\"\n        try:\n            # Try spectral layout first\n            pos = nx.spectral_layout(graph, dim=min(self.dim, len(graph.nodes()) - 1))\n            if pos[list(graph.nodes())[0]].shape[0] &lt; self.dim:\n                # Pad with zeros if needed\n                return {node: np.pad(pos[node], (0, self.dim - pos[node].shape[0])) for node in graph.nodes()}\n            return {node: pos[node] for node in graph.nodes()}\n        except:\n            # Fall back to random initialization if spectral layout fails\n            return {node: np.random.normal(0, 0.1, self.dim) for node in graph.nodes()}\n\n    def _update_embeddings(self, graph: nx.Graph, iterations: int = 50):\n        \"\"\"Update embeddings using force-directed algorithm with graph structure.\"\"\"\n        learning_rate = 0.1\n        repulsion = 0.1\n\n        for _ in range(iterations):\n            # Store old positions\n            old_pos = {node: self.embeddings[node].copy() for node in graph.nodes()}\n\n            # Calculate attractive forces between connected nodes\n            for u, v in graph.edges():\n                delta = self.embeddings[u] - self.embeddings[v]\n                dist = np.linalg.norm(delta)\n                if dist &gt; 0:\n                    force = delta * learning_rate\n                    self.embeddings[u] -= force\n                    self.embeddings[v] += force\n\n            # Calculate repulsive forces between all nodes\n            for u in graph.nodes():\n                force = np.zeros(self.dim)\n                for v in graph.nodes():\n                    if u != v:\n                        delta = self.embeddings[u] - self.embeddings[v]\n                        dist = np.linalg.norm(delta)\n                        if dist &gt; 0:\n                            force += (delta / dist) * repulsion\n                self.embeddings[u] += force\n\n            # Normalize to prevent exploding gradients\n            for node in graph.nodes():\n                norm = np.linalg.norm(self.embeddings[node])\n                if norm &gt; 0:\n                    self.embeddings[node] /= norm\n\n    def compute_distance(self, node1: int, node2: int) -&gt; float:\n        \"\"\"Compute Euclidean distance between two nodes.\"\"\"\n        return float(np.linalg.norm(self.embeddings[node1] - self.embeddings[node2]))\n\n    def get_visualization_coords(self, node: int) -&gt; Tuple[float, float]:\n        \"\"\"Get 2D coordinates for visualization.\"\"\"\n        coords = self.embeddings[node]\n        return float(coords[0]), float(coords[1])\n</code></pre>"},{"location":"api/reference/#core.embeddings.euclidean.EuclideanEmbedding.__init__","title":"<code>__init__(dim=2)</code>","text":"<p>Initialize embedding with dimension.</p> Source code in <code>core/embeddings/euclidean.py</code> <pre><code>def __init__(self, dim: int = 2):\n    \"\"\"Initialize embedding with dimension.\"\"\"\n    super().__init__(dim)\n</code></pre>"},{"location":"api/reference/#core.embeddings.euclidean.EuclideanEmbedding.compute_distance","title":"<code>compute_distance(node1, node2)</code>","text":"<p>Compute Euclidean distance between two nodes.</p> Source code in <code>core/embeddings/euclidean.py</code> <pre><code>def compute_distance(self, node1: int, node2: int) -&gt; float:\n    \"\"\"Compute Euclidean distance between two nodes.\"\"\"\n    return float(np.linalg.norm(self.embeddings[node1] - self.embeddings[node2]))\n</code></pre>"},{"location":"api/reference/#core.embeddings.euclidean.EuclideanEmbedding.get_visualization_coords","title":"<code>get_visualization_coords(node)</code>","text":"<p>Get 2D coordinates for visualization.</p> Source code in <code>core/embeddings/euclidean.py</code> <pre><code>def get_visualization_coords(self, node: int) -&gt; Tuple[float, float]:\n    \"\"\"Get 2D coordinates for visualization.\"\"\"\n    coords = self.embeddings[node]\n    return float(coords[0]), float(coords[1])\n</code></pre>"},{"location":"api/reference/#spherical-embedding","title":"Spherical Embedding","text":"<p>               Bases: <code>BaseEmbedding</code></p> <p>Spherical space embedding using graph structure.</p> Source code in <code>core/embeddings/spherical.py</code> <pre><code>class SphericalEmbedding(BaseEmbedding):\n    \"\"\"Spherical space embedding using graph structure.\"\"\"\n\n    def __init__(self, dim: int = 3):\n        \"\"\"Initialize embedding with dimension (minimum 3 for sphere).\"\"\"\n        super().__init__(max(3, dim))  # Ensure at least 3D for sphere\n\n    def _initialize_embeddings(self, graph: nx.Graph) -&gt; Dict[int, np.ndarray]:\n        \"\"\"Initialize embeddings uniformly on the unit sphere.\"\"\"\n        pos = {}\n\n        # Generate points using Fibonacci sphere method for uniform distribution\n        n = len(graph.nodes())\n        phi = np.pi * (3 - np.sqrt(5))  # golden angle in radians\n\n        for i, node in enumerate(graph.nodes()):\n            y = 1 - (i / float(n - 1)) * 2  # y goes from 1 to -1\n            radius = np.sqrt(1 - y * y)  # radius at y\n\n            theta = phi * i  # golden angle increment\n\n            x = np.cos(theta) * radius\n            z = np.sin(theta) * radius\n\n            # Create the full dimensional vector\n            coords = np.zeros(self.dim)\n            coords[0] = x\n            coords[1] = y\n            coords[2] = z\n\n            pos[node] = coords\n\n        return pos\n\n    def _update_embeddings(self, graph: nx.Graph, iterations: int = 50):\n        \"\"\"Update embeddings using Riemannian optimization on sphere.\"\"\"\n        learning_rate = 0.1\n\n        def _project_to_sphere(v):\n            \"\"\"Project vector onto unit sphere.\"\"\"\n            norm = np.linalg.norm(v)\n            if norm &gt; 0:\n                return v / norm\n            return v\n\n        def _exp_map(x, v):\n            \"\"\"Exponential map on sphere.\"\"\"\n            v_norm = np.linalg.norm(v)\n            if v_norm &lt; 1e-7:\n                return x\n            return np.cos(v_norm) * x + np.sin(v_norm) * v / v_norm\n\n        for _ in range(iterations):\n            # Store old positions\n            old_pos = {node: self.embeddings[node].copy() for node in graph.nodes()}\n\n            # Update each node's position\n            for u in graph.nodes():\n                grad = np.zeros(self.dim)\n\n                # Attractive forces from neighbors\n                for v in graph.neighbors(u):\n                    # Vector in tangent space\n                    diff = old_pos[v] - old_pos[u] * np.dot(old_pos[u], old_pos[v])\n                    grad += diff\n\n                # Repulsive forces from non-neighbors\n                for v in graph.nodes():\n                    if v != u and v not in graph.neighbors(u):\n                        diff = old_pos[v] - old_pos[u] * np.dot(old_pos[u], old_pos[v])\n                        grad -= 0.1 * diff\n\n                # Update position using exponential map\n                tangent_vector = learning_rate * grad\n                self.embeddings[u] = _exp_map(old_pos[u], tangent_vector)\n\n                # Ensure we stay on the sphere\n                self.embeddings[u] = _project_to_sphere(self.embeddings[u])\n\n    def compute_distance(self, node1: int, node2: int) -&gt; float:\n        \"\"\"Compute geodesic distance between two nodes on the sphere.\"\"\"\n        # Get normalized vectors\n        x = self.embeddings[node1]\n        y = self.embeddings[node2]\n\n        # Compute cosine of angle between vectors\n        cos_angle = np.clip(np.dot(x, y), -1.0, 1.0)\n\n        # Return great circle distance\n        return float(np.arccos(cos_angle))\n\n    def get_visualization_coords(self, node: int) -&gt; Tuple[float, float]:\n        \"\"\"Get 2D coordinates for visualization using stereographic projection.\"\"\"\n        x = self.embeddings[node]\n\n        # Use stereographic projection from north pole\n        scale = 1 / (1 + x[2])  # z coordinate is index 2\n        return float(x[0] * scale), float(x[1] * scale)\n</code></pre>"},{"location":"api/reference/#core.embeddings.spherical.SphericalEmbedding.__init__","title":"<code>__init__(dim=3)</code>","text":"<p>Initialize embedding with dimension (minimum 3 for sphere).</p> Source code in <code>core/embeddings/spherical.py</code> <pre><code>def __init__(self, dim: int = 3):\n    \"\"\"Initialize embedding with dimension (minimum 3 for sphere).\"\"\"\n    super().__init__(max(3, dim))  # Ensure at least 3D for sphere\n</code></pre>"},{"location":"api/reference/#core.embeddings.spherical.SphericalEmbedding.compute_distance","title":"<code>compute_distance(node1, node2)</code>","text":"<p>Compute geodesic distance between two nodes on the sphere.</p> Source code in <code>core/embeddings/spherical.py</code> <pre><code>def compute_distance(self, node1: int, node2: int) -&gt; float:\n    \"\"\"Compute geodesic distance between two nodes on the sphere.\"\"\"\n    # Get normalized vectors\n    x = self.embeddings[node1]\n    y = self.embeddings[node2]\n\n    # Compute cosine of angle between vectors\n    cos_angle = np.clip(np.dot(x, y), -1.0, 1.0)\n\n    # Return great circle distance\n    return float(np.arccos(cos_angle))\n</code></pre>"},{"location":"api/reference/#core.embeddings.spherical.SphericalEmbedding.get_visualization_coords","title":"<code>get_visualization_coords(node)</code>","text":"<p>Get 2D coordinates for visualization using stereographic projection.</p> Source code in <code>core/embeddings/spherical.py</code> <pre><code>def get_visualization_coords(self, node: int) -&gt; Tuple[float, float]:\n    \"\"\"Get 2D coordinates for visualization using stereographic projection.\"\"\"\n    x = self.embeddings[node]\n\n    # Use stereographic projection from north pole\n    scale = 1 / (1 + x[2])  # z coordinate is index 2\n    return float(x[0] * scale), float(x[1] * scale)\n</code></pre>"},{"location":"api/reference/#hyperbolic-embedding","title":"Hyperbolic Embedding","text":"<p>               Bases: <code>BaseEmbedding</code></p> <p>Hyperbolic space (Poincar\u00e9 disk) embedding.</p> Source code in <code>core/embeddings/hyperbolic.py</code> <pre><code>class HyperbolicEmbedding(BaseEmbedding):\n    \"\"\"Hyperbolic space (Poincar\u00e9 disk) embedding.\"\"\"\n\n    def __init__(self, dim: int = 2):\n        \"\"\"Initialize embedding with dimension.\"\"\"\n        super().__init__(max(2, dim))  # Ensure at least 2D for Poincar\u00e9 disk\n\n    def _initialize_embeddings(self, graph: nx.Graph) -&gt; Dict[int, np.ndarray]:\n        \"\"\"Initialize embeddings in the Poincar\u00e9 disk using tree-like structure.\"\"\"\n        pos = {}\n        nodes = list(graph.nodes())\n\n        if not nodes:\n            return pos\n\n        # Convert to undirected for component analysis\n        G = graph.to_undirected() if graph.is_directed() else graph\n\n        # Handle disconnected components separately\n        components = list(nx.connected_components(G))\n\n        for component_idx, component in enumerate(components):\n            # Create a subgraph for this component\n            subgraph = graph.subgraph(component)\n\n            # Choose a root node for this component\n            root = list(component)[0]\n\n            # For directed graphs, use weakly connected components\n            if graph.is_directed():\n                bfs_tree = nx.bfs_tree(subgraph.to_undirected(), root)\n            else:\n                bfs_tree = nx.bfs_tree(subgraph, root)\n\n            # Calculate offset for this component to separate it from others\n            offset = np.array([0.3 * component_idx, 0.3 * component_idx] + [0] * (self.dim - 2))\n\n            # Place root with offset\n            pos[root] = offset + np.zeros(self.dim)\n\n            # Place other nodes level by level\n            level = 1\n            nodes_at_level = {0: [root]}\n            current_level_nodes = [root]\n\n            while current_level_nodes:\n                next_level_nodes = []\n                nodes_at_level[level] = []\n\n                for parent in current_level_nodes:\n                    # For directed graphs, consider both in and out neighbors\n                    if graph.is_directed():\n                        children = list(set(bfs_tree.neighbors(parent)) - set(pos.keys()))\n                    else:\n                        children = list(set(bfs_tree.neighbors(parent)) - set(pos.keys()))\n\n                    if not children:\n                        continue\n\n                    # Place children around their parent\n                    angle_step = 2 * np.pi / len(children)\n                    for i, child in enumerate(children):\n                        angle = angle_step * i\n\n                        # Calculate position in polar coordinates\n                        r = np.tanh(0.4 * level)  # Radius increases with level but stays in disk\n                        x = r * np.cos(angle)\n                        y = r * np.sin(angle)\n\n                        # Create position vector\n                        position = np.zeros(self.dim)\n                        position[0] = x\n                        position[1] = y\n\n                        # Add offset and store\n                        pos[child] = offset + position\n                        nodes_at_level[level].append(child)\n                        next_level_nodes.append(child)\n\n                current_level_nodes = next_level_nodes\n                level += 1\n\n        # Initialize any remaining nodes randomly near the origin\n        for node in graph.nodes():\n            if node not in pos:\n                rand_vec = np.random.normal(0, 0.1, self.dim)\n                pos[node] = rand_vec / (np.linalg.norm(rand_vec) + 1e-6) * 0.5\n\n        return pos\n\n    def _update_embeddings(self, graph: nx.Graph, iterations: int = 50):\n        \"\"\"Update embeddings using Riemannian optimization.\"\"\"\n        learning_rate = 0.01\n\n        def _mobius_addition(u, v):\n            \"\"\"M\u00f6bius addition in the Poincar\u00e9 disk.\"\"\"\n            uv = np.sum(u*v)\n            u_norm_sq = np.sum(u*u)\n            v_norm_sq = np.sum(v*v)\n            denominator = 1 + 2*uv + u_norm_sq*v_norm_sq\n            return ((1 + 2*uv + v_norm_sq)*u + (1 - u_norm_sq)*v) / denominator\n\n        def _exp_map(x, v):\n            \"\"\"Exponential map in the Poincar\u00e9 disk.\"\"\"\n            v_norm = np.linalg.norm(v)\n            if v_norm &lt; 1e-7:\n                return x\n            x_norm = np.linalg.norm(x)\n            c = 1 - x_norm**2\n            return _mobius_addition(x, np.tanh(v_norm/c/2) * v/v_norm)\n\n        def _project_to_disk(x):\n            \"\"\"Project point onto Poincar\u00e9 disk.\"\"\"\n            norm = np.linalg.norm(x)\n            if norm &gt;= 1:\n                return x / norm * 0.99\n            return x\n\n        # Ensure all embeddings are in the disk\n        for node in graph.nodes():\n            self.embeddings[node] = _project_to_disk(self.embeddings[node])\n\n        for _ in range(iterations):\n            # Store old positions\n            old_pos = {node: self.embeddings[node].copy() for node in graph.nodes()}\n\n            # Update each node's position\n            for u in graph.nodes():\n                grad = np.zeros(self.dim)\n\n                # For directed graphs, consider both in and out neighbors\n                neighbors = (list(graph.predecessors(u)) + list(graph.successors(u))) if graph.is_directed() else list(graph.neighbors(u))\n                neighbors = list(set(neighbors))  # Remove duplicates\n\n                # Attractive forces from neighbors\n                for v in neighbors:\n                    diff = old_pos[v] - old_pos[u]\n                    dist = self.compute_distance(u, v)\n                    if dist &gt; 0:\n                        grad += diff / (dist + 1e-6)\n\n                # Repulsive forces from non-neighbors\n                for v in graph.nodes():\n                    if v != u and v not in neighbors:\n                        diff = old_pos[v] - old_pos[u]\n                        dist = self.compute_distance(u, v)\n                        if dist &gt; 0:\n                            grad -= 0.1 * diff / (dist + 1e-6)\n\n                # Update position using Riemannian gradient descent\n                self.embeddings[u] = _exp_map(old_pos[u], learning_rate * grad)\n\n                # Project back to Poincar\u00e9 disk\n                self.embeddings[u] = _project_to_disk(self.embeddings[u])\n\n    def compute_distance(self, node1: int, node2: int) -&gt; float:\n        \"\"\"Compute hyperbolic distance between two nodes in the Poincar\u00e9 disk.\"\"\"\n        x = self.embeddings[node1]\n        y = self.embeddings[node2]\n\n        # Compute the hyperbolic distance in the Poincar\u00e9 disk\n        xy = np.sum((x - y) * (x - y))\n        x_norm = np.sum(x * x)\n        y_norm = np.sum(y * y)\n\n        num = 2 * xy\n        den = (1 - x_norm) * (1 - y_norm)\n\n        return float(np.arccosh(1 + num/den))\n\n    def get_visualization_coords(self, node: int) -&gt; Tuple[float, float]:\n        \"\"\"Get 2D coordinates for visualization.\"\"\"\n        coords = self.embeddings[node]\n        return float(coords[0]), float(coords[1])\n</code></pre>"},{"location":"api/reference/#core.embeddings.hyperbolic.HyperbolicEmbedding.__init__","title":"<code>__init__(dim=2)</code>","text":"<p>Initialize embedding with dimension.</p> Source code in <code>core/embeddings/hyperbolic.py</code> <pre><code>def __init__(self, dim: int = 2):\n    \"\"\"Initialize embedding with dimension.\"\"\"\n    super().__init__(max(2, dim))  # Ensure at least 2D for Poincar\u00e9 disk\n</code></pre>"},{"location":"api/reference/#core.embeddings.hyperbolic.HyperbolicEmbedding.compute_distance","title":"<code>compute_distance(node1, node2)</code>","text":"<p>Compute hyperbolic distance between two nodes in the Poincar\u00e9 disk.</p> Source code in <code>core/embeddings/hyperbolic.py</code> <pre><code>def compute_distance(self, node1: int, node2: int) -&gt; float:\n    \"\"\"Compute hyperbolic distance between two nodes in the Poincar\u00e9 disk.\"\"\"\n    x = self.embeddings[node1]\n    y = self.embeddings[node2]\n\n    # Compute the hyperbolic distance in the Poincar\u00e9 disk\n    xy = np.sum((x - y) * (x - y))\n    x_norm = np.sum(x * x)\n    y_norm = np.sum(y * y)\n\n    num = 2 * xy\n    den = (1 - x_norm) * (1 - y_norm)\n\n    return float(np.arccosh(1 + num/den))\n</code></pre>"},{"location":"api/reference/#core.embeddings.hyperbolic.HyperbolicEmbedding.get_visualization_coords","title":"<code>get_visualization_coords(node)</code>","text":"<p>Get 2D coordinates for visualization.</p> Source code in <code>core/embeddings/hyperbolic.py</code> <pre><code>def get_visualization_coords(self, node: int) -&gt; Tuple[float, float]:\n    \"\"\"Get 2D coordinates for visualization.\"\"\"\n    coords = self.embeddings[node]\n    return float(coords[0]), float(coords[1])\n</code></pre>"},{"location":"api/reference/#algorithms","title":"Algorithms","text":""},{"location":"api/reference/#shortest-path-algorithms","title":"Shortest Path Algorithms","text":"<p>Shortest path algorithms with and without embedding awareness.</p>"},{"location":"api/reference/#core.algorithms.shortest_path.DijkstraTraditional","title":"<code>DijkstraTraditional</code>","text":"<p>               Bases: <code>GraphAlgorithm</code></p> <p>Traditional Dijkstra's algorithm implementation.</p> Source code in <code>core/algorithms/shortest_path.py</code> <pre><code>class DijkstraTraditional(GraphAlgorithm):\n    \"\"\"Traditional Dijkstra's algorithm implementation.\"\"\"\n\n    @GraphAlgorithm.measure_execution_time\n    def run(self, source: int, target: int) -&gt; Dict[str, Any]:\n        \"\"\"\n        Find shortest path between source and target nodes.\n\n        Args:\n            source: Source node ID\n            target: Target node ID\n\n        Returns:\n            Dictionary containing:\n                - path: List of nodes in the shortest path\n                - distance: Length of the shortest path\n        \"\"\"\n        distances = {node: float('infinity') for node in self.graph.nodes()}\n        predecessors = {node: None for node in self.graph.nodes()}\n        distances[source] = 0\n        pq = [(0, source)]\n        visited = set()\n\n        while pq:\n            current_distance, current = heapq.heappop(pq)\n\n            if current == target:\n                # Reconstruct path\n                path = []\n                while current is not None:\n                    path.append(current)\n                    current = predecessors[current]\n                path.reverse()\n\n                return {\n                    'path': path,\n                    'distance': distances[target]\n                }\n\n            if current in visited:\n                continue\n\n            visited.add(current)\n\n            for neighbor in self.graph.neighbors(current):\n                distance = current_distance + 1  # Using unit weights\n                if distance &lt; distances[neighbor]:\n                    distances[neighbor] = distance\n                    predecessors[neighbor] = current\n                    heapq.heappush(pq, (distance, neighbor))\n\n        return {\n            'path': [],\n            'distance': float('infinity')\n        }\n</code></pre>"},{"location":"api/reference/#core.algorithms.shortest_path.DijkstraTraditional.run","title":"<code>run(source, target)</code>","text":"<p>Find shortest path between source and target nodes.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>int</code> <p>Source node ID</p> required <code>target</code> <code>int</code> <p>Target node ID</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing: - path: List of nodes in the shortest path - distance: Length of the shortest path</p> Source code in <code>core/algorithms/shortest_path.py</code> <pre><code>@GraphAlgorithm.measure_execution_time\ndef run(self, source: int, target: int) -&gt; Dict[str, Any]:\n    \"\"\"\n    Find shortest path between source and target nodes.\n\n    Args:\n        source: Source node ID\n        target: Target node ID\n\n    Returns:\n        Dictionary containing:\n            - path: List of nodes in the shortest path\n            - distance: Length of the shortest path\n    \"\"\"\n    distances = {node: float('infinity') for node in self.graph.nodes()}\n    predecessors = {node: None for node in self.graph.nodes()}\n    distances[source] = 0\n    pq = [(0, source)]\n    visited = set()\n\n    while pq:\n        current_distance, current = heapq.heappop(pq)\n\n        if current == target:\n            # Reconstruct path\n            path = []\n            while current is not None:\n                path.append(current)\n                current = predecessors[current]\n            path.reverse()\n\n            return {\n                'path': path,\n                'distance': distances[target]\n            }\n\n        if current in visited:\n            continue\n\n        visited.add(current)\n\n        for neighbor in self.graph.neighbors(current):\n            distance = current_distance + 1  # Using unit weights\n            if distance &lt; distances[neighbor]:\n                distances[neighbor] = distance\n                predecessors[neighbor] = current\n                heapq.heappush(pq, (distance, neighbor))\n\n    return {\n        'path': [],\n        'distance': float('infinity')\n    }\n</code></pre>"},{"location":"api/reference/#core.algorithms.shortest_path.DijkstraEmbedding","title":"<code>DijkstraEmbedding</code>","text":"<p>               Bases: <code>EmbeddingAwareAlgorithm</code></p> <p>Embedding-aware Dijkstra's algorithm implementation.</p> <p>This implementation uses embedding distances for path selection, which means it will find different paths than traditional Dijkstra. The paths found will be optimal in the embedding space rather than the original graph space.</p> Source code in <code>core/algorithms/shortest_path.py</code> <pre><code>class DijkstraEmbedding(EmbeddingAwareAlgorithm):\n    \"\"\"Embedding-aware Dijkstra's algorithm implementation.\n\n    This implementation uses embedding distances for path selection,\n    which means it will find different paths than traditional Dijkstra.\n    The paths found will be optimal in the embedding space rather than\n    the original graph space.\n    \"\"\"\n\n    def _get_edge_weight(self, u: int, v: int) -&gt; float:\n        \"\"\"\n        Get the weight between two nodes based on their embedding distance.\n\n        Args:\n            u: First node ID\n            v: Second node ID\n\n        Returns:\n            Edge weight based on embedding distance\n        \"\"\"\n        return self.embedder.compute_distance(u, v)\n\n    @GraphAlgorithm.measure_execution_time\n    def run(self, source: int, target: int) -&gt; Dict[str, Any]:\n        \"\"\"\n        Find shortest path between source and target nodes using embedding distances\n        for path selection. This will find paths that are optimal in the embedding\n        space rather than the original graph space.\n\n        Args:\n            source: Source node ID\n            target: Target node ID\n\n        Returns:\n            Dictionary containing:\n                - path: List of nodes in the shortest path\n                - distance: Length of the shortest path\n        \"\"\"\n        distances = {node: float('infinity') for node in self.graph.nodes()}\n        predecessors = {node: None for node in self.graph.nodes()}\n        distances[source] = 0\n        pq = [(0, source)]\n        visited = set()\n\n        while pq:\n            current_distance, current = heapq.heappop(pq)\n\n            if current == target:\n                # Reconstruct path\n                path = []\n                while current is not None:\n                    path.append(current)\n                    current = predecessors[current]\n                path.reverse()\n\n                return {\n                    'path': path,\n                    'distance': distances[target]\n                }\n\n            if current in visited:\n                continue\n\n            visited.add(current)\n\n            for neighbor in self.graph.neighbors(current):\n                weight = self._get_edge_weight(current, neighbor)\n                distance = current_distance + weight\n\n                if distance &lt; distances[neighbor]:\n                    distances[neighbor] = distance\n                    predecessors[neighbor] = current\n                    heapq.heappush(pq, (distance, neighbor))\n\n        return {\n            'path': [],\n            'distance': float('infinity')\n        }\n</code></pre>"},{"location":"api/reference/#core.algorithms.shortest_path.DijkstraEmbedding.run","title":"<code>run(source, target)</code>","text":"<p>Find shortest path between source and target nodes using embedding distances for path selection. This will find paths that are optimal in the embedding space rather than the original graph space.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>int</code> <p>Source node ID</p> required <code>target</code> <code>int</code> <p>Target node ID</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing: - path: List of nodes in the shortest path - distance: Length of the shortest path</p> Source code in <code>core/algorithms/shortest_path.py</code> <pre><code>@GraphAlgorithm.measure_execution_time\ndef run(self, source: int, target: int) -&gt; Dict[str, Any]:\n    \"\"\"\n    Find shortest path between source and target nodes using embedding distances\n    for path selection. This will find paths that are optimal in the embedding\n    space rather than the original graph space.\n\n    Args:\n        source: Source node ID\n        target: Target node ID\n\n    Returns:\n        Dictionary containing:\n            - path: List of nodes in the shortest path\n            - distance: Length of the shortest path\n    \"\"\"\n    distances = {node: float('infinity') for node in self.graph.nodes()}\n    predecessors = {node: None for node in self.graph.nodes()}\n    distances[source] = 0\n    pq = [(0, source)]\n    visited = set()\n\n    while pq:\n        current_distance, current = heapq.heappop(pq)\n\n        if current == target:\n            # Reconstruct path\n            path = []\n            while current is not None:\n                path.append(current)\n                current = predecessors[current]\n            path.reverse()\n\n            return {\n                'path': path,\n                'distance': distances[target]\n            }\n\n        if current in visited:\n            continue\n\n        visited.add(current)\n\n        for neighbor in self.graph.neighbors(current):\n            weight = self._get_edge_weight(current, neighbor)\n            distance = current_distance + weight\n\n            if distance &lt; distances[neighbor]:\n                distances[neighbor] = distance\n                predecessors[neighbor] = current\n                heapq.heappush(pq, (distance, neighbor))\n\n    return {\n        'path': [],\n        'distance': float('infinity')\n    }\n</code></pre>"},{"location":"api/algorithms/shortest_path/","title":"Shortest Path Algorithms","text":""},{"location":"api/algorithms/shortest_path/#core.algorithms.shortest_path","title":"<code>core.algorithms.shortest_path</code>","text":"<p>Shortest path algorithms with and without embedding awareness.</p>"},{"location":"api/algorithms/shortest_path/#core.algorithms.shortest_path.DijkstraTraditional","title":"<code>DijkstraTraditional</code>","text":"<p>               Bases: <code>GraphAlgorithm</code></p> <p>Traditional Dijkstra's algorithm implementation.</p> Source code in <code>core/algorithms/shortest_path.py</code> <pre><code>class DijkstraTraditional(GraphAlgorithm):\n    \"\"\"Traditional Dijkstra's algorithm implementation.\"\"\"\n\n    @GraphAlgorithm.measure_execution_time\n    def run(self, source: int, target: int) -&gt; Dict[str, Any]:\n        \"\"\"\n        Find shortest path between source and target nodes.\n\n        Args:\n            source: Source node ID\n            target: Target node ID\n\n        Returns:\n            Dictionary containing:\n                - path: List of nodes in the shortest path\n                - distance: Length of the shortest path\n        \"\"\"\n        distances = {node: float('infinity') for node in self.graph.nodes()}\n        predecessors = {node: None for node in self.graph.nodes()}\n        distances[source] = 0\n        pq = [(0, source)]\n        visited = set()\n\n        while pq:\n            current_distance, current = heapq.heappop(pq)\n\n            if current == target:\n                # Reconstruct path\n                path = []\n                while current is not None:\n                    path.append(current)\n                    current = predecessors[current]\n                path.reverse()\n\n                return {\n                    'path': path,\n                    'distance': distances[target]\n                }\n\n            if current in visited:\n                continue\n\n            visited.add(current)\n\n            for neighbor in self.graph.neighbors(current):\n                distance = current_distance + 1  # Using unit weights\n                if distance &lt; distances[neighbor]:\n                    distances[neighbor] = distance\n                    predecessors[neighbor] = current\n                    heapq.heappush(pq, (distance, neighbor))\n\n        return {\n            'path': [],\n            'distance': float('infinity')\n        }\n</code></pre>"},{"location":"api/algorithms/shortest_path/#core.algorithms.shortest_path.DijkstraTraditional.run","title":"<code>run(source, target)</code>","text":"<p>Find shortest path between source and target nodes.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>int</code> <p>Source node ID</p> required <code>target</code> <code>int</code> <p>Target node ID</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing: - path: List of nodes in the shortest path - distance: Length of the shortest path</p> Source code in <code>core/algorithms/shortest_path.py</code> <pre><code>@GraphAlgorithm.measure_execution_time\ndef run(self, source: int, target: int) -&gt; Dict[str, Any]:\n    \"\"\"\n    Find shortest path between source and target nodes.\n\n    Args:\n        source: Source node ID\n        target: Target node ID\n\n    Returns:\n        Dictionary containing:\n            - path: List of nodes in the shortest path\n            - distance: Length of the shortest path\n    \"\"\"\n    distances = {node: float('infinity') for node in self.graph.nodes()}\n    predecessors = {node: None for node in self.graph.nodes()}\n    distances[source] = 0\n    pq = [(0, source)]\n    visited = set()\n\n    while pq:\n        current_distance, current = heapq.heappop(pq)\n\n        if current == target:\n            # Reconstruct path\n            path = []\n            while current is not None:\n                path.append(current)\n                current = predecessors[current]\n            path.reverse()\n\n            return {\n                'path': path,\n                'distance': distances[target]\n            }\n\n        if current in visited:\n            continue\n\n        visited.add(current)\n\n        for neighbor in self.graph.neighbors(current):\n            distance = current_distance + 1  # Using unit weights\n            if distance &lt; distances[neighbor]:\n                distances[neighbor] = distance\n                predecessors[neighbor] = current\n                heapq.heappush(pq, (distance, neighbor))\n\n    return {\n        'path': [],\n        'distance': float('infinity')\n    }\n</code></pre>"},{"location":"api/algorithms/shortest_path/#core.algorithms.shortest_path.DijkstraEmbedding","title":"<code>DijkstraEmbedding</code>","text":"<p>               Bases: <code>EmbeddingAwareAlgorithm</code></p> <p>Embedding-aware Dijkstra's algorithm implementation.</p> <p>This implementation uses embedding distances for path selection, which means it will find different paths than traditional Dijkstra. The paths found will be optimal in the embedding space rather than the original graph space.</p> Source code in <code>core/algorithms/shortest_path.py</code> <pre><code>class DijkstraEmbedding(EmbeddingAwareAlgorithm):\n    \"\"\"Embedding-aware Dijkstra's algorithm implementation.\n\n    This implementation uses embedding distances for path selection,\n    which means it will find different paths than traditional Dijkstra.\n    The paths found will be optimal in the embedding space rather than\n    the original graph space.\n    \"\"\"\n\n    def _get_edge_weight(self, u: int, v: int) -&gt; float:\n        \"\"\"\n        Get the weight between two nodes based on their embedding distance.\n\n        Args:\n            u: First node ID\n            v: Second node ID\n\n        Returns:\n            Edge weight based on embedding distance\n        \"\"\"\n        return self.embedder.compute_distance(u, v)\n\n    @GraphAlgorithm.measure_execution_time\n    def run(self, source: int, target: int) -&gt; Dict[str, Any]:\n        \"\"\"\n        Find shortest path between source and target nodes using embedding distances\n        for path selection. This will find paths that are optimal in the embedding\n        space rather than the original graph space.\n\n        Args:\n            source: Source node ID\n            target: Target node ID\n\n        Returns:\n            Dictionary containing:\n                - path: List of nodes in the shortest path\n                - distance: Length of the shortest path\n        \"\"\"\n        distances = {node: float('infinity') for node in self.graph.nodes()}\n        predecessors = {node: None for node in self.graph.nodes()}\n        distances[source] = 0\n        pq = [(0, source)]\n        visited = set()\n\n        while pq:\n            current_distance, current = heapq.heappop(pq)\n\n            if current == target:\n                # Reconstruct path\n                path = []\n                while current is not None:\n                    path.append(current)\n                    current = predecessors[current]\n                path.reverse()\n\n                return {\n                    'path': path,\n                    'distance': distances[target]\n                }\n\n            if current in visited:\n                continue\n\n            visited.add(current)\n\n            for neighbor in self.graph.neighbors(current):\n                weight = self._get_edge_weight(current, neighbor)\n                distance = current_distance + weight\n\n                if distance &lt; distances[neighbor]:\n                    distances[neighbor] = distance\n                    predecessors[neighbor] = current\n                    heapq.heappush(pq, (distance, neighbor))\n\n        return {\n            'path': [],\n            'distance': float('infinity')\n        }\n</code></pre>"},{"location":"api/algorithms/shortest_path/#core.algorithms.shortest_path.DijkstraEmbedding.run","title":"<code>run(source, target)</code>","text":"<p>Find shortest path between source and target nodes using embedding distances for path selection. This will find paths that are optimal in the embedding space rather than the original graph space.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>int</code> <p>Source node ID</p> required <code>target</code> <code>int</code> <p>Target node ID</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing: - path: List of nodes in the shortest path - distance: Length of the shortest path</p> Source code in <code>core/algorithms/shortest_path.py</code> <pre><code>@GraphAlgorithm.measure_execution_time\ndef run(self, source: int, target: int) -&gt; Dict[str, Any]:\n    \"\"\"\n    Find shortest path between source and target nodes using embedding distances\n    for path selection. This will find paths that are optimal in the embedding\n    space rather than the original graph space.\n\n    Args:\n        source: Source node ID\n        target: Target node ID\n\n    Returns:\n        Dictionary containing:\n            - path: List of nodes in the shortest path\n            - distance: Length of the shortest path\n    \"\"\"\n    distances = {node: float('infinity') for node in self.graph.nodes()}\n    predecessors = {node: None for node in self.graph.nodes()}\n    distances[source] = 0\n    pq = [(0, source)]\n    visited = set()\n\n    while pq:\n        current_distance, current = heapq.heappop(pq)\n\n        if current == target:\n            # Reconstruct path\n            path = []\n            while current is not None:\n                path.append(current)\n                current = predecessors[current]\n            path.reverse()\n\n            return {\n                'path': path,\n                'distance': distances[target]\n            }\n\n        if current in visited:\n            continue\n\n        visited.add(current)\n\n        for neighbor in self.graph.neighbors(current):\n            weight = self._get_edge_weight(current, neighbor)\n            distance = current_distance + weight\n\n            if distance &lt; distances[neighbor]:\n                distances[neighbor] = distance\n                predecessors[neighbor] = current\n                heapq.heappush(pq, (distance, neighbor))\n\n    return {\n        'path': [],\n        'distance': float('infinity')\n    }\n</code></pre>"},{"location":"api/embeddings/base/","title":"Base Embedding","text":""},{"location":"api/embeddings/base/#core.embeddings.base.BaseEmbedding","title":"<code>core.embeddings.base.BaseEmbedding</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all embedding types.</p> Source code in <code>core/embeddings/base.py</code> <pre><code>class BaseEmbedding(ABC):\n    \"\"\"Base class for all embedding types.\"\"\"\n\n    def __init__(self, dim: int = 2):\n        \"\"\"Initialize embedding parameters.\"\"\"\n        self.dim = dim\n        self.embeddings = {}\n\n    def train(self, graph: nx.Graph, iterations: int = 50) -&gt; None:\n        \"\"\"Train the embeddings on a graph.\"\"\"\n        # Initialize embeddings for all nodes\n        init_embeddings = self._initialize_embeddings(graph)\n\n        # Ensure all nodes have embeddings\n        for node in graph.nodes():\n            if node not in init_embeddings:\n                # Initialize missing nodes with random values\n                init_embeddings[node] = np.random.normal(0, 0.1, self.dim)\n\n        self.embeddings = init_embeddings\n\n        # Update embeddings\n        self._update_embeddings(graph, iterations)\n\n        # Final check to ensure all nodes have embeddings\n        for node in graph.nodes():\n            if node not in self.embeddings:\n                raise ValueError(f\"Node {node} missing from embeddings after training\")\n\n    @abstractmethod\n    def _initialize_embeddings(self, graph: nx.Graph) -&gt; Dict[int, np.ndarray]:\n        \"\"\"Initialize embeddings for all nodes in the graph.\"\"\"\n        pass\n\n    @abstractmethod\n    def _update_embeddings(self, graph: nx.Graph, iterations: int) -&gt; None:\n        \"\"\"Update embeddings using graph structure.\"\"\"\n        pass\n\n    def get_embedding(self, node: int) -&gt; np.ndarray:\n        \"\"\"Get the embedding vector for a node.\"\"\"\n        return self.embeddings[node]\n\n    @abstractmethod\n    def compute_distance(self, node1: int, node2: int) -&gt; float:\n        \"\"\"Compute distance between two nodes in the embedding space.\"\"\"\n        pass\n\n    @abstractmethod\n    def get_visualization_coords(self, node: int) -&gt; Tuple[float, float]:\n        \"\"\"Get 2D coordinates for visualization.\"\"\"\n        pass\n</code></pre>"},{"location":"api/embeddings/base/#core.embeddings.base.BaseEmbedding.__init__","title":"<code>__init__(dim=2)</code>","text":"<p>Initialize embedding parameters.</p> Source code in <code>core/embeddings/base.py</code> <pre><code>def __init__(self, dim: int = 2):\n    \"\"\"Initialize embedding parameters.\"\"\"\n    self.dim = dim\n    self.embeddings = {}\n</code></pre>"},{"location":"api/embeddings/base/#core.embeddings.base.BaseEmbedding.compute_distance","title":"<code>compute_distance(node1, node2)</code>  <code>abstractmethod</code>","text":"<p>Compute distance between two nodes in the embedding space.</p> Source code in <code>core/embeddings/base.py</code> <pre><code>@abstractmethod\ndef compute_distance(self, node1: int, node2: int) -&gt; float:\n    \"\"\"Compute distance between two nodes in the embedding space.\"\"\"\n    pass\n</code></pre>"},{"location":"api/embeddings/base/#core.embeddings.base.BaseEmbedding.get_embedding","title":"<code>get_embedding(node)</code>","text":"<p>Get the embedding vector for a node.</p> Source code in <code>core/embeddings/base.py</code> <pre><code>def get_embedding(self, node: int) -&gt; np.ndarray:\n    \"\"\"Get the embedding vector for a node.\"\"\"\n    return self.embeddings[node]\n</code></pre>"},{"location":"api/embeddings/base/#core.embeddings.base.BaseEmbedding.get_visualization_coords","title":"<code>get_visualization_coords(node)</code>  <code>abstractmethod</code>","text":"<p>Get 2D coordinates for visualization.</p> Source code in <code>core/embeddings/base.py</code> <pre><code>@abstractmethod\ndef get_visualization_coords(self, node: int) -&gt; Tuple[float, float]:\n    \"\"\"Get 2D coordinates for visualization.\"\"\"\n    pass\n</code></pre>"},{"location":"api/embeddings/base/#core.embeddings.base.BaseEmbedding.train","title":"<code>train(graph, iterations=50)</code>","text":"<p>Train the embeddings on a graph.</p> Source code in <code>core/embeddings/base.py</code> <pre><code>def train(self, graph: nx.Graph, iterations: int = 50) -&gt; None:\n    \"\"\"Train the embeddings on a graph.\"\"\"\n    # Initialize embeddings for all nodes\n    init_embeddings = self._initialize_embeddings(graph)\n\n    # Ensure all nodes have embeddings\n    for node in graph.nodes():\n        if node not in init_embeddings:\n            # Initialize missing nodes with random values\n            init_embeddings[node] = np.random.normal(0, 0.1, self.dim)\n\n    self.embeddings = init_embeddings\n\n    # Update embeddings\n    self._update_embeddings(graph, iterations)\n\n    # Final check to ensure all nodes have embeddings\n    for node in graph.nodes():\n        if node not in self.embeddings:\n            raise ValueError(f\"Node {node} missing from embeddings after training\")\n</code></pre>"},{"location":"api/embeddings/euclidean/","title":"Euclidean Embedding","text":""},{"location":"api/embeddings/euclidean/#core.embeddings.euclidean.EuclideanEmbedding","title":"<code>core.embeddings.euclidean.EuclideanEmbedding</code>","text":"<p>               Bases: <code>BaseEmbedding</code></p> <p>Euclidean space embedding using graph structure.</p> Source code in <code>core/embeddings/euclidean.py</code> <pre><code>class EuclideanEmbedding(BaseEmbedding):\n    \"\"\"Euclidean space embedding using graph structure.\"\"\"\n\n    def __init__(self, dim: int = 2):\n        \"\"\"Initialize embedding with dimension.\"\"\"\n        super().__init__(dim)\n\n    def _initialize_embeddings(self, graph: nx.Graph) -&gt; Dict[int, np.ndarray]:\n        \"\"\"Initialize node embeddings using spectral layout for better starting positions.\"\"\"\n        try:\n            # Try spectral layout first\n            pos = nx.spectral_layout(graph, dim=min(self.dim, len(graph.nodes()) - 1))\n            if pos[list(graph.nodes())[0]].shape[0] &lt; self.dim:\n                # Pad with zeros if needed\n                return {node: np.pad(pos[node], (0, self.dim - pos[node].shape[0])) for node in graph.nodes()}\n            return {node: pos[node] for node in graph.nodes()}\n        except:\n            # Fall back to random initialization if spectral layout fails\n            return {node: np.random.normal(0, 0.1, self.dim) for node in graph.nodes()}\n\n    def _update_embeddings(self, graph: nx.Graph, iterations: int = 50):\n        \"\"\"Update embeddings using force-directed algorithm with graph structure.\"\"\"\n        learning_rate = 0.1\n        repulsion = 0.1\n\n        for _ in range(iterations):\n            # Store old positions\n            old_pos = {node: self.embeddings[node].copy() for node in graph.nodes()}\n\n            # Calculate attractive forces between connected nodes\n            for u, v in graph.edges():\n                delta = self.embeddings[u] - self.embeddings[v]\n                dist = np.linalg.norm(delta)\n                if dist &gt; 0:\n                    force = delta * learning_rate\n                    self.embeddings[u] -= force\n                    self.embeddings[v] += force\n\n            # Calculate repulsive forces between all nodes\n            for u in graph.nodes():\n                force = np.zeros(self.dim)\n                for v in graph.nodes():\n                    if u != v:\n                        delta = self.embeddings[u] - self.embeddings[v]\n                        dist = np.linalg.norm(delta)\n                        if dist &gt; 0:\n                            force += (delta / dist) * repulsion\n                self.embeddings[u] += force\n\n            # Normalize to prevent exploding gradients\n            for node in graph.nodes():\n                norm = np.linalg.norm(self.embeddings[node])\n                if norm &gt; 0:\n                    self.embeddings[node] /= norm\n\n    def compute_distance(self, node1: int, node2: int) -&gt; float:\n        \"\"\"Compute Euclidean distance between two nodes.\"\"\"\n        return float(np.linalg.norm(self.embeddings[node1] - self.embeddings[node2]))\n\n    def get_visualization_coords(self, node: int) -&gt; Tuple[float, float]:\n        \"\"\"Get 2D coordinates for visualization.\"\"\"\n        coords = self.embeddings[node]\n        return float(coords[0]), float(coords[1])\n</code></pre>"},{"location":"api/embeddings/euclidean/#core.embeddings.euclidean.EuclideanEmbedding.__init__","title":"<code>__init__(dim=2)</code>","text":"<p>Initialize embedding with dimension.</p> Source code in <code>core/embeddings/euclidean.py</code> <pre><code>def __init__(self, dim: int = 2):\n    \"\"\"Initialize embedding with dimension.\"\"\"\n    super().__init__(dim)\n</code></pre>"},{"location":"api/embeddings/euclidean/#core.embeddings.euclidean.EuclideanEmbedding.compute_distance","title":"<code>compute_distance(node1, node2)</code>","text":"<p>Compute Euclidean distance between two nodes.</p> Source code in <code>core/embeddings/euclidean.py</code> <pre><code>def compute_distance(self, node1: int, node2: int) -&gt; float:\n    \"\"\"Compute Euclidean distance between two nodes.\"\"\"\n    return float(np.linalg.norm(self.embeddings[node1] - self.embeddings[node2]))\n</code></pre>"},{"location":"api/embeddings/euclidean/#core.embeddings.euclidean.EuclideanEmbedding.get_visualization_coords","title":"<code>get_visualization_coords(node)</code>","text":"<p>Get 2D coordinates for visualization.</p> Source code in <code>core/embeddings/euclidean.py</code> <pre><code>def get_visualization_coords(self, node: int) -&gt; Tuple[float, float]:\n    \"\"\"Get 2D coordinates for visualization.\"\"\"\n    coords = self.embeddings[node]\n    return float(coords[0]), float(coords[1])\n</code></pre>"},{"location":"api/embeddings/hyperbolic/","title":"Hyperbolic Embedding","text":""},{"location":"api/embeddings/hyperbolic/#core.embeddings.hyperbolic.HyperbolicEmbedding","title":"<code>core.embeddings.hyperbolic.HyperbolicEmbedding</code>","text":"<p>               Bases: <code>BaseEmbedding</code></p> <p>Hyperbolic space (Poincar\u00e9 disk) embedding.</p> Source code in <code>core/embeddings/hyperbolic.py</code> <pre><code>class HyperbolicEmbedding(BaseEmbedding):\n    \"\"\"Hyperbolic space (Poincar\u00e9 disk) embedding.\"\"\"\n\n    def __init__(self, dim: int = 2):\n        \"\"\"Initialize embedding with dimension.\"\"\"\n        super().__init__(max(2, dim))  # Ensure at least 2D for Poincar\u00e9 disk\n\n    def _initialize_embeddings(self, graph: nx.Graph) -&gt; Dict[int, np.ndarray]:\n        \"\"\"Initialize embeddings in the Poincar\u00e9 disk using tree-like structure.\"\"\"\n        pos = {}\n        nodes = list(graph.nodes())\n\n        if not nodes:\n            return pos\n\n        # Convert to undirected for component analysis\n        G = graph.to_undirected() if graph.is_directed() else graph\n\n        # Handle disconnected components separately\n        components = list(nx.connected_components(G))\n\n        for component_idx, component in enumerate(components):\n            # Create a subgraph for this component\n            subgraph = graph.subgraph(component)\n\n            # Choose a root node for this component\n            root = list(component)[0]\n\n            # For directed graphs, use weakly connected components\n            if graph.is_directed():\n                bfs_tree = nx.bfs_tree(subgraph.to_undirected(), root)\n            else:\n                bfs_tree = nx.bfs_tree(subgraph, root)\n\n            # Calculate offset for this component to separate it from others\n            offset = np.array([0.3 * component_idx, 0.3 * component_idx] + [0] * (self.dim - 2))\n\n            # Place root with offset\n            pos[root] = offset + np.zeros(self.dim)\n\n            # Place other nodes level by level\n            level = 1\n            nodes_at_level = {0: [root]}\n            current_level_nodes = [root]\n\n            while current_level_nodes:\n                next_level_nodes = []\n                nodes_at_level[level] = []\n\n                for parent in current_level_nodes:\n                    # For directed graphs, consider both in and out neighbors\n                    if graph.is_directed():\n                        children = list(set(bfs_tree.neighbors(parent)) - set(pos.keys()))\n                    else:\n                        children = list(set(bfs_tree.neighbors(parent)) - set(pos.keys()))\n\n                    if not children:\n                        continue\n\n                    # Place children around their parent\n                    angle_step = 2 * np.pi / len(children)\n                    for i, child in enumerate(children):\n                        angle = angle_step * i\n\n                        # Calculate position in polar coordinates\n                        r = np.tanh(0.4 * level)  # Radius increases with level but stays in disk\n                        x = r * np.cos(angle)\n                        y = r * np.sin(angle)\n\n                        # Create position vector\n                        position = np.zeros(self.dim)\n                        position[0] = x\n                        position[1] = y\n\n                        # Add offset and store\n                        pos[child] = offset + position\n                        nodes_at_level[level].append(child)\n                        next_level_nodes.append(child)\n\n                current_level_nodes = next_level_nodes\n                level += 1\n\n        # Initialize any remaining nodes randomly near the origin\n        for node in graph.nodes():\n            if node not in pos:\n                rand_vec = np.random.normal(0, 0.1, self.dim)\n                pos[node] = rand_vec / (np.linalg.norm(rand_vec) + 1e-6) * 0.5\n\n        return pos\n\n    def _update_embeddings(self, graph: nx.Graph, iterations: int = 50):\n        \"\"\"Update embeddings using Riemannian optimization.\"\"\"\n        learning_rate = 0.01\n\n        def _mobius_addition(u, v):\n            \"\"\"M\u00f6bius addition in the Poincar\u00e9 disk.\"\"\"\n            uv = np.sum(u*v)\n            u_norm_sq = np.sum(u*u)\n            v_norm_sq = np.sum(v*v)\n            denominator = 1 + 2*uv + u_norm_sq*v_norm_sq\n            return ((1 + 2*uv + v_norm_sq)*u + (1 - u_norm_sq)*v) / denominator\n\n        def _exp_map(x, v):\n            \"\"\"Exponential map in the Poincar\u00e9 disk.\"\"\"\n            v_norm = np.linalg.norm(v)\n            if v_norm &lt; 1e-7:\n                return x\n            x_norm = np.linalg.norm(x)\n            c = 1 - x_norm**2\n            return _mobius_addition(x, np.tanh(v_norm/c/2) * v/v_norm)\n\n        def _project_to_disk(x):\n            \"\"\"Project point onto Poincar\u00e9 disk.\"\"\"\n            norm = np.linalg.norm(x)\n            if norm &gt;= 1:\n                return x / norm * 0.99\n            return x\n\n        # Ensure all embeddings are in the disk\n        for node in graph.nodes():\n            self.embeddings[node] = _project_to_disk(self.embeddings[node])\n\n        for _ in range(iterations):\n            # Store old positions\n            old_pos = {node: self.embeddings[node].copy() for node in graph.nodes()}\n\n            # Update each node's position\n            for u in graph.nodes():\n                grad = np.zeros(self.dim)\n\n                # For directed graphs, consider both in and out neighbors\n                neighbors = (list(graph.predecessors(u)) + list(graph.successors(u))) if graph.is_directed() else list(graph.neighbors(u))\n                neighbors = list(set(neighbors))  # Remove duplicates\n\n                # Attractive forces from neighbors\n                for v in neighbors:\n                    diff = old_pos[v] - old_pos[u]\n                    dist = self.compute_distance(u, v)\n                    if dist &gt; 0:\n                        grad += diff / (dist + 1e-6)\n\n                # Repulsive forces from non-neighbors\n                for v in graph.nodes():\n                    if v != u and v not in neighbors:\n                        diff = old_pos[v] - old_pos[u]\n                        dist = self.compute_distance(u, v)\n                        if dist &gt; 0:\n                            grad -= 0.1 * diff / (dist + 1e-6)\n\n                # Update position using Riemannian gradient descent\n                self.embeddings[u] = _exp_map(old_pos[u], learning_rate * grad)\n\n                # Project back to Poincar\u00e9 disk\n                self.embeddings[u] = _project_to_disk(self.embeddings[u])\n\n    def compute_distance(self, node1: int, node2: int) -&gt; float:\n        \"\"\"Compute hyperbolic distance between two nodes in the Poincar\u00e9 disk.\"\"\"\n        x = self.embeddings[node1]\n        y = self.embeddings[node2]\n\n        # Compute the hyperbolic distance in the Poincar\u00e9 disk\n        xy = np.sum((x - y) * (x - y))\n        x_norm = np.sum(x * x)\n        y_norm = np.sum(y * y)\n\n        num = 2 * xy\n        den = (1 - x_norm) * (1 - y_norm)\n\n        return float(np.arccosh(1 + num/den))\n\n    def get_visualization_coords(self, node: int) -&gt; Tuple[float, float]:\n        \"\"\"Get 2D coordinates for visualization.\"\"\"\n        coords = self.embeddings[node]\n        return float(coords[0]), float(coords[1])\n</code></pre>"},{"location":"api/embeddings/hyperbolic/#core.embeddings.hyperbolic.HyperbolicEmbedding.__init__","title":"<code>__init__(dim=2)</code>","text":"<p>Initialize embedding with dimension.</p> Source code in <code>core/embeddings/hyperbolic.py</code> <pre><code>def __init__(self, dim: int = 2):\n    \"\"\"Initialize embedding with dimension.\"\"\"\n    super().__init__(max(2, dim))  # Ensure at least 2D for Poincar\u00e9 disk\n</code></pre>"},{"location":"api/embeddings/hyperbolic/#core.embeddings.hyperbolic.HyperbolicEmbedding.compute_distance","title":"<code>compute_distance(node1, node2)</code>","text":"<p>Compute hyperbolic distance between two nodes in the Poincar\u00e9 disk.</p> Source code in <code>core/embeddings/hyperbolic.py</code> <pre><code>def compute_distance(self, node1: int, node2: int) -&gt; float:\n    \"\"\"Compute hyperbolic distance between two nodes in the Poincar\u00e9 disk.\"\"\"\n    x = self.embeddings[node1]\n    y = self.embeddings[node2]\n\n    # Compute the hyperbolic distance in the Poincar\u00e9 disk\n    xy = np.sum((x - y) * (x - y))\n    x_norm = np.sum(x * x)\n    y_norm = np.sum(y * y)\n\n    num = 2 * xy\n    den = (1 - x_norm) * (1 - y_norm)\n\n    return float(np.arccosh(1 + num/den))\n</code></pre>"},{"location":"api/embeddings/hyperbolic/#core.embeddings.hyperbolic.HyperbolicEmbedding.get_visualization_coords","title":"<code>get_visualization_coords(node)</code>","text":"<p>Get 2D coordinates for visualization.</p> Source code in <code>core/embeddings/hyperbolic.py</code> <pre><code>def get_visualization_coords(self, node: int) -&gt; Tuple[float, float]:\n    \"\"\"Get 2D coordinates for visualization.\"\"\"\n    coords = self.embeddings[node]\n    return float(coords[0]), float(coords[1])\n</code></pre>"},{"location":"api/embeddings/spherical/","title":"Spherical Embedding","text":""},{"location":"api/embeddings/spherical/#core.embeddings.spherical.SphericalEmbedding","title":"<code>core.embeddings.spherical.SphericalEmbedding</code>","text":"<p>               Bases: <code>BaseEmbedding</code></p> <p>Spherical space embedding using graph structure.</p> Source code in <code>core/embeddings/spherical.py</code> <pre><code>class SphericalEmbedding(BaseEmbedding):\n    \"\"\"Spherical space embedding using graph structure.\"\"\"\n\n    def __init__(self, dim: int = 3):\n        \"\"\"Initialize embedding with dimension (minimum 3 for sphere).\"\"\"\n        super().__init__(max(3, dim))  # Ensure at least 3D for sphere\n\n    def _initialize_embeddings(self, graph: nx.Graph) -&gt; Dict[int, np.ndarray]:\n        \"\"\"Initialize embeddings uniformly on the unit sphere.\"\"\"\n        pos = {}\n\n        # Generate points using Fibonacci sphere method for uniform distribution\n        n = len(graph.nodes())\n        phi = np.pi * (3 - np.sqrt(5))  # golden angle in radians\n\n        for i, node in enumerate(graph.nodes()):\n            y = 1 - (i / float(n - 1)) * 2  # y goes from 1 to -1\n            radius = np.sqrt(1 - y * y)  # radius at y\n\n            theta = phi * i  # golden angle increment\n\n            x = np.cos(theta) * radius\n            z = np.sin(theta) * radius\n\n            # Create the full dimensional vector\n            coords = np.zeros(self.dim)\n            coords[0] = x\n            coords[1] = y\n            coords[2] = z\n\n            pos[node] = coords\n\n        return pos\n\n    def _update_embeddings(self, graph: nx.Graph, iterations: int = 50):\n        \"\"\"Update embeddings using Riemannian optimization on sphere.\"\"\"\n        learning_rate = 0.1\n\n        def _project_to_sphere(v):\n            \"\"\"Project vector onto unit sphere.\"\"\"\n            norm = np.linalg.norm(v)\n            if norm &gt; 0:\n                return v / norm\n            return v\n\n        def _exp_map(x, v):\n            \"\"\"Exponential map on sphere.\"\"\"\n            v_norm = np.linalg.norm(v)\n            if v_norm &lt; 1e-7:\n                return x\n            return np.cos(v_norm) * x + np.sin(v_norm) * v / v_norm\n\n        for _ in range(iterations):\n            # Store old positions\n            old_pos = {node: self.embeddings[node].copy() for node in graph.nodes()}\n\n            # Update each node's position\n            for u in graph.nodes():\n                grad = np.zeros(self.dim)\n\n                # Attractive forces from neighbors\n                for v in graph.neighbors(u):\n                    # Vector in tangent space\n                    diff = old_pos[v] - old_pos[u] * np.dot(old_pos[u], old_pos[v])\n                    grad += diff\n\n                # Repulsive forces from non-neighbors\n                for v in graph.nodes():\n                    if v != u and v not in graph.neighbors(u):\n                        diff = old_pos[v] - old_pos[u] * np.dot(old_pos[u], old_pos[v])\n                        grad -= 0.1 * diff\n\n                # Update position using exponential map\n                tangent_vector = learning_rate * grad\n                self.embeddings[u] = _exp_map(old_pos[u], tangent_vector)\n\n                # Ensure we stay on the sphere\n                self.embeddings[u] = _project_to_sphere(self.embeddings[u])\n\n    def compute_distance(self, node1: int, node2: int) -&gt; float:\n        \"\"\"Compute geodesic distance between two nodes on the sphere.\"\"\"\n        # Get normalized vectors\n        x = self.embeddings[node1]\n        y = self.embeddings[node2]\n\n        # Compute cosine of angle between vectors\n        cos_angle = np.clip(np.dot(x, y), -1.0, 1.0)\n\n        # Return great circle distance\n        return float(np.arccos(cos_angle))\n\n    def get_visualization_coords(self, node: int) -&gt; Tuple[float, float]:\n        \"\"\"Get 2D coordinates for visualization using stereographic projection.\"\"\"\n        x = self.embeddings[node]\n\n        # Use stereographic projection from north pole\n        scale = 1 / (1 + x[2])  # z coordinate is index 2\n        return float(x[0] * scale), float(x[1] * scale)\n</code></pre>"},{"location":"api/embeddings/spherical/#core.embeddings.spherical.SphericalEmbedding.__init__","title":"<code>__init__(dim=3)</code>","text":"<p>Initialize embedding with dimension (minimum 3 for sphere).</p> Source code in <code>core/embeddings/spherical.py</code> <pre><code>def __init__(self, dim: int = 3):\n    \"\"\"Initialize embedding with dimension (minimum 3 for sphere).\"\"\"\n    super().__init__(max(3, dim))  # Ensure at least 3D for sphere\n</code></pre>"},{"location":"api/embeddings/spherical/#core.embeddings.spherical.SphericalEmbedding.compute_distance","title":"<code>compute_distance(node1, node2)</code>","text":"<p>Compute geodesic distance between two nodes on the sphere.</p> Source code in <code>core/embeddings/spherical.py</code> <pre><code>def compute_distance(self, node1: int, node2: int) -&gt; float:\n    \"\"\"Compute geodesic distance between two nodes on the sphere.\"\"\"\n    # Get normalized vectors\n    x = self.embeddings[node1]\n    y = self.embeddings[node2]\n\n    # Compute cosine of angle between vectors\n    cos_angle = np.clip(np.dot(x, y), -1.0, 1.0)\n\n    # Return great circle distance\n    return float(np.arccos(cos_angle))\n</code></pre>"},{"location":"api/embeddings/spherical/#core.embeddings.spherical.SphericalEmbedding.get_visualization_coords","title":"<code>get_visualization_coords(node)</code>","text":"<p>Get 2D coordinates for visualization using stereographic projection.</p> Source code in <code>core/embeddings/spherical.py</code> <pre><code>def get_visualization_coords(self, node: int) -&gt; Tuple[float, float]:\n    \"\"\"Get 2D coordinates for visualization using stereographic projection.\"\"\"\n    x = self.embeddings[node]\n\n    # Use stereographic projection from north pole\n    scale = 1 / (1 + x[2])  # z coordinate is index 2\n    return float(x[0] * scale), float(x[1] * scale)\n</code></pre>"},{"location":"benchmarks/performance/","title":"Performance Benchmarks","text":"<p>This section presents comprehensive benchmarks comparing traditional graph algorithms with their embedding-based counterparts.</p>"},{"location":"benchmarks/performance/#benchmark-results","title":"Benchmark Results","text":"<p>Our benchmarking suite measures both time and memory performance across different: - Graph types (Erd\u0151s-R\u00e9nyi, Barab\u00e1si-Albert, etc.) - Graph sizes (100 to 10,000 nodes) - Algorithms (PageRank, Node Classification, Community Detection, Link Prediction)</p>"},{"location":"benchmarks/performance/#time-performance","title":"Time Performance","text":"<p>The time performance plots show the speedup factor achieved by using embeddings: - Speedup Factor = Traditional Algorithm Time / Embedding-Based Algorithm Time - Values &gt; 1 indicate that the embedding-based approach is faster - Higher values indicate better performance</p> <p></p>"},{"location":"benchmarks/performance/#memory-usage","title":"Memory Usage","text":"<p>The memory reduction plots show how much memory is saved by using embeddings: - Memory Reduction = Traditional Algorithm Memory / Embedding-Based Algorithm Memory - Values &gt; 1 indicate that the embedding-based approach uses less memory - Higher values indicate better memory efficiency</p> <p></p>"},{"location":"benchmarks/performance/#scaling-behavior","title":"Scaling Behavior","text":"<p>The scaling plots show how performance changes with graph size: - X-axis: Number of nodes in the graph (log scale) - Y-axis: Speedup/Memory reduction factor (log scale)</p> <p>This helps visualize how embedding-based algorithms maintain their advantage as graphs grow larger.</p>"},{"location":"benchmarks/performance/#time-scaling","title":"Time Scaling","text":""},{"location":"benchmarks/performance/#memory-scaling","title":"Memory Scaling","text":""},{"location":"benchmarks/performance/#performance-heatmaps","title":"Performance Heatmaps","text":"<p>The heatmaps provide a comprehensive view of performance across different: - Graph types - Graph sizes - Algorithms</p> <p>This helps identify which scenarios benefit most from using embeddings.</p>"},{"location":"benchmarks/performance/#time-performance-heatmap","title":"Time Performance Heatmap","text":""},{"location":"benchmarks/performance/#memory-performance-heatmap","title":"Memory Performance Heatmap","text":""},{"location":"benchmarks/performance/#running-benchmarks","title":"Running Benchmarks","text":"<p>To run the benchmarks yourself:</p> <pre><code># Run the benchmarks\npython benchmarks/algorithm_benchmarks.py\n\n# Generate visualizations\npython benchmarks/visualize_results.py\n</code></pre> <p>The results will be saved in <code>benchmark_results/</code> with interactive HTML plots in <code>benchmark_results/plots/</code>.</p>"},{"location":"examples/advanced/","title":"Advanced Usage","text":"<p>This page demonstrates advanced usage patterns and techniques for the Graph Embeddings library.</p>"},{"location":"examples/advanced/#custom-distance-functions","title":"Custom Distance Functions","text":"<p>You can create custom embeddings by subclassing <code>BaseEmbedding</code> and implementing your own distance function:</p> <pre><code>import numpy as np\nfrom core.embeddings import BaseEmbedding\n\nclass CustomEmbedding(BaseEmbedding):\n    def compute_distance(self, node1: int, node2: int) -&gt; float:\n        \"\"\"Custom distance function using Manhattan distance.\"\"\"\n        v1 = self.embeddings[node1]\n        v2 = self.embeddings[node2]\n        return np.sum(np.abs(v1 - v2))\n\n    def _initialize_embeddings(self, graph):\n        \"\"\"Custom initialization strategy.\"\"\"\n        pos = {}\n        for node in graph.nodes():\n            pos[node] = np.random.uniform(-1, 1, self.dim)\n        return pos\n</code></pre>"},{"location":"examples/advanced/#working-with-large-graphs","title":"Working with Large Graphs","text":"<p>For large graphs, you can use batching and optimization techniques:</p> <pre><code>import networkx as nx\nfrom core.embeddings import EuclideanEmbedding\n\n# Create a large graph\nG = nx.barabasi_albert_graph(10000, 3)\n\n# Initialize embedding with optimization parameters\nembedding = EuclideanEmbedding(\n    dim=2,\n    batch_size=128,\n    learning_rate=0.01,\n    num_epochs=50\n)\n\n# Train with progress tracking\nembedding.train(G, verbose=True)\n</code></pre>"},{"location":"examples/advanced/#embedding-evaluation","title":"Embedding Evaluation","text":"<p>Evaluate embedding quality using various metrics:</p> <pre><code>from core.evaluation import (\n    compute_stress,\n    compute_distortion,\n    evaluate_link_prediction\n)\n\n# Compute stress (how well distances are preserved)\nstress = compute_stress(G, embedding)\nprint(f\"Embedding stress: {stress}\")\n\n# Compute distortion\ndistortion = compute_distortion(G, embedding)\nprint(f\"Average distortion: {distortion}\")\n\n# Evaluate link prediction\nauc_score = evaluate_link_prediction(G, embedding)\nprint(f\"Link prediction AUC: {auc_score}\")\n</code></pre>"},{"location":"examples/advanced/#combining-multiple-embeddings","title":"Combining Multiple Embeddings","text":"<p>Create ensemble embeddings by combining different types:</p> <pre><code>from core.embeddings import EuclideanEmbedding, HyperbolicEmbedding\nimport numpy as np\n\nclass EnsembleEmbedding:\n    def __init__(self, embeddings, weights=None):\n        self.embeddings = embeddings\n        self.weights = weights or [1/len(embeddings)] * len(embeddings)\n\n    def compute_distance(self, node1, node2):\n        distances = [\n            emb.compute_distance(node1, node2)\n            for emb in self.embeddings\n        ]\n        return np.average(distances, weights=self.weights)\n\n# Create ensemble\neuclidean = EuclideanEmbedding(dim=2)\nhyperbolic = HyperbolicEmbedding(dim=2)\n\neuclidean.train(G)\nhyperbolic.train(G)\n\nensemble = EnsembleEmbedding(\n    embeddings=[euclidean, hyperbolic],\n    weights=[0.7, 0.3]\n)\n</code></pre>"},{"location":"examples/advanced/#custom-visualization","title":"Custom Visualization","text":"<p>Create custom visualizations using the embedding coordinates:</p> <pre><code>import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\ndef plot_3d_embedding(G, embedding):\n    \"\"\"Create a 3D visualization of the embedding.\"\"\"\n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(111, projection='3d')\n\n    # Get node positions\n    pos = {\n        node: embedding.get_embedding(node)\n        for node in G.nodes()\n    }\n\n    # Plot nodes\n    xs = [pos[node][0] for node in G.nodes()]\n    ys = [pos[node][1] for node in G.nodes()]\n    zs = [pos[node][2] for node in G.nodes()]\n    ax.scatter(xs, ys, zs)\n\n    # Plot edges\n    for edge in G.edges():\n        x = [pos[edge[0]][0], pos[edge[1]][0]]\n        y = [pos[edge[0]][1], pos[edge[1]][1]]\n        z = [pos[edge[0]][2], pos[edge[1]][2]]\n        ax.plot(x, y, z, 'gray', alpha=0.5)\n\n    plt.show()\n\n# Use the custom visualization\nspherical = SphericalEmbedding(dim=3)\nspherical.train(G)\nplot_3d_embedding(G, spherical)\n</code></pre>"},{"location":"examples/basic/","title":"Basic Examples","text":"<p>This page contains basic examples of using the Graph Embeddings library.</p>"},{"location":"examples/basic/#creating-and-training-embeddings","title":"Creating and Training Embeddings","text":"<pre><code>import networkx as nx\nfrom core.embeddings import EuclideanEmbedding\n\n# Create a sample graph\nG = nx.karate_club_graph()\n\n# Initialize embedding\nembedding = EuclideanEmbedding(dim=2)\n\n# Train the embedding\nembedding.train(G)\n\n# Access node embeddings\nnode_0_embedding = embedding.get_embedding(0)\nprint(f\"Node 0 embedding: {node_0_embedding}\")\n</code></pre>"},{"location":"examples/basic/#finding-shortest-paths","title":"Finding Shortest Paths","text":"<pre><code>from core.algorithms import DijkstraEmbedding, DijkstraTraditional\n\n# Initialize algorithms\ndijkstra_embedding = DijkstraEmbedding(embedding)\ndijkstra_traditional = DijkstraTraditional()\n\n# Find shortest path using embedding-aware algorithm\npath_embedding = dijkstra_embedding.shortest_path(0, 33)\nprint(f\"Embedding-aware path: {path_embedding}\")\n\n# Compare with traditional algorithm\npath_traditional = dijkstra_traditional.shortest_path(G, 0, 33)\nprint(f\"Traditional path: {path_traditional}\")\n</code></pre>"},{"location":"examples/basic/#different-embedding-types","title":"Different Embedding Types","text":""},{"location":"examples/basic/#spherical-embedding","title":"Spherical Embedding","text":"<pre><code>from core.embeddings import SphericalEmbedding\n\n# Initialize and train spherical embedding\nspherical = SphericalEmbedding(dim=3)  # Must be 3 dimensions for sphere\nspherical.train(G)\n\n# Access embeddings\nnode_embeddings = {node: spherical.get_embedding(node) for node in G.nodes()}\n</code></pre>"},{"location":"examples/basic/#hyperbolic-embedding","title":"Hyperbolic Embedding","text":"<pre><code>from core.embeddings import HyperbolicEmbedding\n\n# Initialize and train hyperbolic embedding\nhyperbolic = HyperbolicEmbedding(dim=2)  # 2D Poincar\u00e9 disk\nhyperbolic.train(G)\n\n# Compute distance between nodes\ndistance = hyperbolic.compute_distance(0, 1)\nprint(f\"Hyperbolic distance between nodes 0 and 1: {distance}\")\n</code></pre>"},{"location":"examples/basic/#visualization","title":"Visualization","text":"<pre><code>import streamlit as st\nfrom core.visualization import visualize_graph\n\n# Create tabs for different visualizations\ntab1, tab2, tab3 = st.tabs([\"Euclidean\", \"Spherical\", \"Hyperbolic\"])\n\nwith tab1:\n    st.header(\"Euclidean Embedding\")\n    visualize_graph(G, euclidean_embedding)\n\nwith tab2:\n    st.header(\"Spherical Embedding\")\n    visualize_graph(G, spherical_embedding)\n\nwith tab3:\n    st.header(\"Hyperbolic Embedding\")\n    visualize_graph(G, hyperbolic_embedding)\n</code></pre>"},{"location":"guide/getting-started/","title":"Getting Started","text":"<p>This guide will help you get started with the Graph Embeddings library.</p>"},{"location":"guide/getting-started/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.8 or higher</li> <li>NetworkX</li> <li>NumPy</li> <li>Streamlit (for visualization)</li> </ul>"},{"location":"guide/getting-started/#installation","title":"Installation","text":"<p>Install the library using pip:</p> <pre><code>pip install -r requirements.txt\n</code></pre>"},{"location":"guide/getting-started/#basic-usage","title":"Basic Usage","text":""},{"location":"guide/getting-started/#1-creating-an-embedding","title":"1. Creating an Embedding","text":"<pre><code>from core.embeddings import EuclideanEmbedding\nimport networkx as nx\n\n# Create a sample graph\nG = nx.karate_club_graph()\n\n# Initialize the embedding\nembedding = EuclideanEmbedding(dim=2)\n\n# Train the embedding\nembedding.train(G)\n</code></pre>"},{"location":"guide/getting-started/#2-using-embedding-aware-algorithms","title":"2. Using Embedding-Aware Algorithms","text":"<pre><code>from core.algorithms import DijkstraEmbedding\n\n# Initialize the algorithm with the embedding\ndijkstra = DijkstraEmbedding(embedding)\n\n# Find shortest path\npath = dijkstra.shortest_path(source=0, target=33)\nprint(f\"Shortest path: {path}\")\n</code></pre>"},{"location":"guide/getting-started/#3-visualizing-results","title":"3. Visualizing Results","text":"<pre><code>import streamlit as st\nfrom core.visualization import visualize_graph\n\n# Visualize the graph with embeddings\nst.title(\"Graph Visualization\")\nvisualize_graph(G, embedding)\n</code></pre>"},{"location":"guide/getting-started/#choosing-an-embedding-type","title":"Choosing an Embedding Type","text":"<p>The library provides three types of embeddings:</p> <ol> <li>Euclidean Embedding: Best for general-purpose graph embedding</li> <li>Spherical Embedding: Suitable for graphs with hierarchical structure</li> <li>Hyperbolic Embedding: Optimal for scale-free networks</li> </ol> <p>Choose the embedding type based on your graph's properties and your specific needs.</p>"},{"location":"guide/getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about advanced usage</li> <li>Explore the API reference</li> <li>Check out example notebooks</li> </ul>"},{"location":"guide/installation/","title":"Installation Guide","text":""},{"location":"guide/installation/#system-requirements","title":"System Requirements","text":"<ul> <li>Python 3.8 or higher</li> <li>pip package manager</li> <li>Virtual environment (recommended)</li> </ul>"},{"location":"guide/installation/#dependencies","title":"Dependencies","text":"<p>The Graph Embeddings library requires the following main dependencies:</p> <ul> <li>NetworkX: For graph operations</li> <li>NumPy: For numerical computations</li> <li>Streamlit: For interactive visualization</li> <li>SciPy: For scientific computing</li> <li>Matplotlib: For plotting</li> </ul>"},{"location":"guide/installation/#installation-steps","title":"Installation Steps","text":""},{"location":"guide/installation/#1-create-a-virtual-environment-recommended","title":"1. Create a Virtual Environment (Recommended)","text":"<pre><code># Create a new virtual environment\npython -m venv venv\n\n# Activate the virtual environment\n# On macOS/Linux:\nsource venv/bin/activate\n# On Windows:\n.\\venv\\Scripts\\activate\n</code></pre>"},{"location":"guide/installation/#2-install-from-source","title":"2. Install from Source","text":"<p>Clone the repository and install the package:</p> <pre><code># Clone the repository\ngit clone https://github.com/yourusername/graph-embeddings.git\ncd graph-embeddings\n\n# Install dependencies\npip install -r requirements.txt\n</code></pre>"},{"location":"guide/installation/#3-verify-installation","title":"3. Verify Installation","text":"<pre><code># Test the installation\npython -c \"from core.embeddings import EuclideanEmbedding; print('Installation successful!')\"\n</code></pre>"},{"location":"guide/installation/#optional-dependencies","title":"Optional Dependencies","text":"<p>For advanced features, you may want to install additional packages:</p> <pre><code># For GPU support\npip install torch\n\n# For advanced visualization\npip install plotly\n\n# For documentation\npip install mkdocs-material mkdocstrings[python] mkdocs-jupyter\n</code></pre>"},{"location":"guide/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"guide/installation/#common-issues","title":"Common Issues","text":"<ol> <li>ImportError: No module named 'core'</li> <li>Make sure you're in the correct directory</li> <li>Check if the package is installed correctly</li> <li> <p>Verify your Python path</p> </li> <li> <p>Version conflicts</p> </li> <li>Try creating a fresh virtual environment</li> <li> <p>Update all dependencies to their latest versions</p> </li> <li> <p>GPU-related errors</p> </li> <li>Ensure CUDA is installed correctly</li> <li>Check GPU compatibility</li> <li>Verify PyTorch installation</li> </ol>"},{"location":"guide/installation/#getting-help","title":"Getting Help","text":"<p>If you encounter any issues:</p> <ol> <li>Check the GitHub Issues</li> <li>Join our Discord community</li> <li>Create a new issue with:</li> <li>Your system information</li> <li>Error message</li> <li>Steps to reproduce</li> </ol>"},{"location":"theory/algorithms/","title":"Graph Algorithms with Embeddings","text":""},{"location":"theory/algorithms/#shortest-path-algorithms","title":"Shortest Path Algorithms","text":""},{"location":"theory/algorithms/#traditional-dijkstras-algorithm","title":"Traditional Dijkstra's Algorithm","text":"<p>The classical Dijkstra's algorithm finds shortest paths in a weighted graph \\(G=(V,E)\\) with non-negative edge weights \\(w: E \\rightarrow \\mathbb{R}^+\\).</p>"},{"location":"theory/algorithms/#algorithm-description","title":"Algorithm Description","text":"<p>For source vertex \\(s\\): 1. Initialize distances: \\(d[v] = \\infty\\) for all \\(v \\neq s\\), \\(d[s] = 0\\) 2. Initialize priority queue \\(Q\\) with \\((s,0)\\) 3. While \\(Q\\) not empty:    - Extract vertex \\(u\\) with minimum distance    - For each neighbor \\(v\\) of \\(u\\):      - If \\(d[v] &gt; d[u] + w(u,v)\\):        - \\(d[v] = d[u] + w(u,v)\\)        - Update \\(v\\) in \\(Q\\)</p> <p>Time complexity: \\(O((|V| + |E|)\\log|V|)\\) with binary heap</p>"},{"location":"theory/algorithms/#embedding-aware-dijkstras-algorithm","title":"Embedding-Aware Dijkstra's Algorithm","text":"<p>This variant leverages geometric information from embeddings to improve path finding.</p>"},{"location":"theory/algorithms/#mathematical-foundation","title":"Mathematical Foundation","text":"<p>Given an embedding \\(f: V \\rightarrow X\\) where \\(X\\) is the embedding space:</p> <ol> <li> <p>Edge Weights: Combine graph and geometric distances:    \\(\\(w'(u,v) = \\alpha w(u,v) + (1-\\alpha)d_X(f(u), f(v))\\)\\)    where \\(d_X\\) is the distance in embedding space and \\(\\alpha \\in [0,1]\\)</p> </li> <li> <p>Heuristic Function: Use embedding distance as heuristic:    \\(\\(h(v,t) = \\beta d_X(f(v), f(t))\\)\\)    where \\(t\\) is the target vertex and \\(\\beta\\) is a scaling factor</p> </li> </ol>"},{"location":"theory/algorithms/#algorithm-description_1","title":"Algorithm Description","text":"<ol> <li>Initialize as in traditional Dijkstra</li> <li>Modify priority function to include heuristic:    <pre><code>Priority = d[v] + h(v,t)\n</code></pre></li> <li>Use modified edge weights \\(w'(u,v)\\)</li> </ol>"},{"location":"theory/algorithms/#theoretical-properties","title":"Theoretical Properties","text":"<ol> <li>Admissibility:</li> <li>If \\(\\beta d_X(f(u), f(v)) \\leq d_G(u,v)\\) for all \\(u,v\\)</li> <li> <p>Then algorithm finds optimal paths</p> </li> <li> <p>Consistency:</p> </li> <li>If \\(h(u,t) \\leq w'(u,v) + h(v,t)\\) for all edges \\((u,v)\\)</li> <li>Then algorithm explores minimum number of nodes</li> </ol>"},{"location":"theory/algorithms/#distance-computation","title":"Distance Computation","text":""},{"location":"theory/algorithms/#embedding-space-distances","title":"Embedding Space Distances","text":"<p>For vertices \\(u,v \\in V\\), compute \\(d_X(f(u), f(v))\\) based on embedding type:</p> <ol> <li>Euclidean:</li> </ol> <p>\\(d_E(x,y) = \\sqrt{\\sum_{i=1}^d (x_i - y_i)^2}\\)</p> <ol> <li>Spherical:</li> </ol> <p>\\(d_S(x,y) = \\arccos\\left(\\frac{\\langle x,y \\rangle}{\\|x\\|\\|y\\|}\\right)\\)</p> <ol> <li>Hyperbolic (Poincar\u00e9 model):</li> </ol> <p>\\(d_H(x,y) = \\text{arcosh}\\left(1 + 2\\frac{\\|x-y\\|^2}{(1-\\|x\\|^2)(1-\\|y\\|^2)}\\right)\\)</p>"},{"location":"theory/algorithms/#graph-distances","title":"Graph Distances","text":"<p>Multiple approaches for computing graph distances:</p> <ol> <li>All-Pairs Shortest Paths:</li> <li>Floyd-Warshall: \\(O(|V|^3)\\)</li> <li> <p>Johnson's: \\(O(|V||E| + |V|^2\\log|V|)\\)</p> </li> <li> <p>Approximate Distances:</p> </li> <li>Landmark-based: Select \\(k\\) landmarks</li> <li>Estimate \\(d(u,v) \\approx \\min_{\\ell \\in L} (d(u,\\ell) + d(\\ell,v))\\)</li> <li>Complexity: \\(O(k|V|)\\) after preprocessing</li> </ol>"},{"location":"theory/algorithms/#performance-analysis","title":"Performance Analysis","text":""},{"location":"theory/algorithms/#time-complexity","title":"Time Complexity","text":"Algorithm Traditional Embedding-Aware Preprocessing O(1) O(V d) Query O((V+E)\\log V) O((V+E)\\log V) Distance Computation O(1) O(d) <p>where \\(d\\) is embedding dimension</p>"},{"location":"theory/algorithms/#space-complexity","title":"Space Complexity","text":"Component Memory Usage Graph O(V + E) Embeddings O(V d) Priority Queue O(V)"},{"location":"theory/algorithms/#practical-considerations","title":"Practical Considerations","text":"<ol> <li>Embedding Dimension:</li> <li>Higher \\(d\\) \u2192 Better accuracy</li> <li>Higher \\(d\\) \u2192 More computation</li> <li> <p>Typical range: 8-128</p> </li> <li> <p>Parameter Tuning:</p> </li> <li>\\(\\alpha\\): Balance between graph and geometric information</li> <li>\\(\\beta\\): Control heuristic strength</li> <li> <p>Cross-validation on path lengths</p> </li> <li> <p>Optimization:</p> </li> <li>Cache frequently computed distances</li> <li>Use approximate nearest neighbors</li> <li>Parallel computation of distances</li> </ol>"},{"location":"theory/embeddings/","title":"Mathematical Background of Graph Embeddings","text":""},{"location":"theory/embeddings/#introduction-to-graph-embeddings","title":"Introduction to Graph Embeddings","text":"<p>Graph embeddings map nodes from a graph into a continuous vector space while preserving structural information. Formally, given a graph G = (V, E), we want to find a mapping:</p> \\[f: V \\to \\mathbb{R}^d\\] <p>where d is the dimension of the embedding space.</p> <p>The key challenge is preserving graph distances in the embedded space. For nodes u, v \u2208 V, we want:</p> \\[d_G(u,v) \\approx d_E(f(u), f(v))\\] <p>where \\(d_G\\) is the graph distance and \\(d_E\\) is the distance in the embedding space.</p>"},{"location":"theory/embeddings/#euclidean-embeddings","title":"Euclidean Embeddings","text":""},{"location":"theory/embeddings/#theory","title":"Theory","text":"<p>Euclidean embeddings map nodes into standard Euclidean space \\(\\mathbb{R}^d\\) with the Euclidean distance metric:</p> \\[d_E(x, y) = \\sqrt{\\sum_{i=1}^d (x_i - y_i)^2}\\] <p>Properties: - Symmetry: \\(d(x,y) = d(y,x)\\) - Triangle Inequality: \\(d(x,z) \\leq d(x,y) + d(y,z)\\) - Translation Invariance: Distance preserved under translations</p>"},{"location":"theory/embeddings/#limitations","title":"Limitations","text":"<p>Euclidean space has constant curvature = 0, which can limit its ability to represent hierarchical structures. The number of points that can be embedded at a fixed distance from a central point grows polynomially with the radius.</p>"},{"location":"theory/embeddings/#spherical-embeddings","title":"Spherical Embeddings","text":""},{"location":"theory/embeddings/#theory_1","title":"Theory","text":"<p>Spherical embeddings map nodes onto the surface of a \\(d\\)-dimensional sphere \\(\\mathbb{S}^{d-1}\\). Points are represented as unit vectors, with distance measured by great circle distance or angular distance:</p> \\[d_S(x, y) = \\arccos(\\langle x, y \\rangle)\\] <p>where \\(\\langle x, y \\rangle\\) is the dot product of unit vectors.</p> <p>Properties: - Positive Curvature: The space has constant positive curvature - Bounded: All distances are bounded by \\(\\pi\\) - Symmetry: Distances are symmetric and rotationally invariant</p>"},{"location":"theory/embeddings/#applications","title":"Applications","text":"<p>Particularly useful for: - Data with inherent spherical structure - Problems requiring bounded distances - Circular or periodic relationships</p>"},{"location":"theory/embeddings/#hyperbolic-embeddings","title":"Hyperbolic Embeddings","text":""},{"location":"theory/embeddings/#theory_2","title":"Theory","text":"<p>Hyperbolic embeddings map nodes into hyperbolic space \\(\\mathbb{H}^d\\), which has constant negative curvature. We typically use the Poincar\u00e9 ball model:</p> \\[d_H(x, y) = \\text{arcosh}\\left(1 + 2\\frac{\\|x-y\\|^2}{(1-\\|x\\|^2)(1-\\|y\\|^2)}\\right)\\] <p>Properties: - Negative Curvature: Space expands exponentially - Tree-Like: Natural representation for hierarchical structures - Exponential Growth: The space available at distance \\(r\\) grows exponentially with \\(r\\)</p>"},{"location":"theory/embeddings/#advantages","title":"Advantages","text":"<p>Hyperbolic geometry is particularly well-suited for: - Hierarchical structures - Scale-free networks - Trees and tree-like graphs</p> <p>The volume of a ball in hyperbolic space grows exponentially with its radius, allowing efficient embedding of hierarchical structures.</p>"},{"location":"theory/embeddings/#comparison-of-embedding-spaces","title":"Comparison of Embedding Spaces","text":""},{"location":"theory/embeddings/#geometric-properties","title":"Geometric Properties","text":"Property Euclidean Spherical Hyperbolic Curvature 0 +1 -1 Volume Growth Polynomial Bounded Exponential Distance Type Unbounded Bounded Unbounded Best For Flat structures Circular patterns Hierarchies"},{"location":"theory/embeddings/#distortion-bounds","title":"Distortion Bounds","text":"<p>For a graph G with n nodes:</p> <ol> <li>Euclidean Space:</li> <li>Some graphs require \\(\\Omega(\\log n)\\) distortion</li> <li> <p>Complete binary trees require \\(\\Omega(\\log n)\\) dimensions</p> </li> <li> <p>Spherical Space:</p> </li> <li>All distances bounded by \\(\\pi\\)</li> <li> <p>Best for graphs with bounded diameter</p> </li> <li> <p>Hyperbolic Space:</p> </li> <li>Trees can be embedded with zero distortion</li> <li>Many real-world networks can be embedded in low dimensions</li> </ol>"},{"location":"theory/embeddings/#implementation-considerations","title":"Implementation Considerations","text":""},{"location":"theory/embeddings/#optimization-objective","title":"Optimization Objective","text":"<p>The general form of the optimization objective is:</p> \\[\\min_f \\sum_{(u,v) \\in E} \\|d_G(u,v) - d_E(f(u), f(v))\\|^2\\] <p>Additional terms might include:</p> <ol> <li> <p>Stress term:    \\(\\(\\sum_{u,v \\in V} (d_G(u,v) - d_E(f(u), f(v)))^2\\)\\)</p> </li> <li> <p>Regularization:    \\(\\(\\lambda \\sum_{u \\in V} \\|f(u)\\|^2\\)\\)</p> </li> <li> <p>Negative sampling:    \\(\\(\\sum_{(u,v) \\not\\in E} \\max(0, \\gamma - d_E(f(u), f(v)))\\)\\)</p> </li> </ol>"},{"location":"theory/embeddings/#initialization-strategies","title":"Initialization Strategies","text":"<ol> <li>Random Initialization:</li> <li>Euclidean: Sample from \\(\\mathcal{N}(0, \\sigma^2)\\)</li> <li>Spherical: Normalize random vectors</li> <li> <p>Hyperbolic: Sample from truncated normal in Poincar\u00e9 ball</p> </li> <li> <p>Spectral Initialization:</p> </li> <li>Use top-\\(d\\) eigenvectors of graph Laplacian</li> <li> <p>Project onto appropriate manifold if needed</p> </li> <li> <p>Landmark-based:</p> </li> <li>Choose landmark nodes</li> <li>Initialize other nodes based on distances to landmarks</li> </ol>"}]}